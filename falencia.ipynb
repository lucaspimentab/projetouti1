{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "520a08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexão estabelecida.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  database \"mimiciv\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.35, but the operating system provides version 2.39.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE mimiciv REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n"
     ]
    }
   ],
   "source": [
    "# Célula 1 - Importações e conexão ao banco\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mimiciv\",\n",
    "    user=\"uti_user\",\n",
    "    password=\"s0f4C1#4\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "print(\"✅ Conexão estabelecida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28c20514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total admissões na UTI (>1h): 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51965/829476087.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_total = pd.read_sql(\"SELECT COUNT(*) AS total_admissoes FROM todas_utis;\", conn)\n"
     ]
    }
   ],
   "source": [
    "# Célula 2 - Criar coorte UTI > 1h\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"ROLLBACK\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TEMP TABLE todas_utis AS\n",
    "SELECT \n",
    "    i.subject_id,\n",
    "    i.hadm_id,\n",
    "    i.stay_id,\n",
    "    i.intime,\n",
    "    i.outtime,\n",
    "    EXTRACT(EPOCH FROM (i.outtime - i.intime)) / 60 AS duracao_minutos\n",
    "FROM mimiciv_icu.icustays i\n",
    "WHERE EXTRACT(EPOCH FROM (i.outtime - i.intime)) > 3600\n",
    "  AND i.stay_id IN (\n",
    "      SELECT DISTINCT stay_id\n",
    "      FROM mimiciv_derived.vasoactive_agent\n",
    "  )\n",
    "LIMIT 50;\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "df_total = pd.read_sql(\"SELECT COUNT(*) AS total_admissoes FROM todas_utis;\", conn)\n",
    "print(f\"✅ Total admissões na UTI (>1h): {df_total['total_admissoes'][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1b4930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51965/3020859792.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_vitals = pd.read_sql(query_vitals, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sinais vitais imputados com método adaptativo do HiRID e máscaras criadas.\n"
     ]
    }
   ],
   "source": [
    "query_vitals = \"\"\"\n",
    "SELECT\n",
    "    vs.stay_id,\n",
    "    vs.charttime,\n",
    "    vs.heart_rate,\n",
    "    vs.sbp,\n",
    "    vs.dbp,\n",
    "    vs.mbp,\n",
    "    vs.resp_rate,\n",
    "    vs.temperature,\n",
    "    vs.spo2,\n",
    "    vs.glucose\n",
    "FROM mimiciv_derived.vitalsign vs\n",
    "WHERE vs.stay_id IN (SELECT stay_id FROM todas_utis)\n",
    "ORDER BY vs.stay_id, vs.charttime;\n",
    "\"\"\"\n",
    "\n",
    "df_vitals = pd.read_sql(query_vitals, conn)\n",
    "df_vitals['charttime'] = pd.to_datetime(df_vitals['charttime'])\n",
    "df_vitals = df_vitals.sort_values(['stay_id', 'charttime'])\n",
    "\n",
    "df_5min = (\n",
    "    df_vitals\n",
    "    .set_index('charttime')\n",
    "    .groupby('stay_id', group_keys=False)\n",
    "    .resample('5min')\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_5min['stay_id'] = df_5min['stay_id'].ffill()\n",
    "\n",
    "sinais_vitais = ['heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate', 'temperature', 'spo2', 'glucose']\n",
    "\n",
    "def calcular_intervalos_amostragem(df, variaveis, tempo_col='charttime', id_col='stay_id'):\n",
    "    parametros = {}\n",
    "    for var in variaveis:\n",
    "        diffs_totais = []\n",
    "        for _, g in df.groupby(id_col):\n",
    "            tempos = g[tempo_col].dropna().sort_values()\n",
    "            if len(tempos) > 1:\n",
    "                diffs = tempos.diff().dt.total_seconds().dropna()\n",
    "                diffs_totais.extend(diffs.values)\n",
    "        if diffs_totais:\n",
    "            mediana = np.median(diffs_totais)\n",
    "            iqr = np.percentile(diffs_totais, 75) - np.percentile(diffs_totais, 25)\n",
    "            parametros[var] = (mediana, iqr)\n",
    "        else:\n",
    "            parametros[var] = (3600, 1800)\n",
    "    return parametros\n",
    "\n",
    "NORMAL_VALUES = {\n",
    "    'heart_rate': 80,\n",
    "    'sbp': 120,\n",
    "    'dbp': 80,\n",
    "    'mbp': 90,\n",
    "    'resp_rate': 16,\n",
    "    'temperature': 37,\n",
    "    'spo2': 98,\n",
    "    'glucose': 100,\n",
    "}\n",
    "\n",
    "parametros = calcular_intervalos_amostragem(df_5min, sinais_vitais)\n",
    "\n",
    "def imputacao_adaptativa(df, variaveis, parametros, normal_values, tempo_col='charttime', id_col='stay_id'):\n",
    "    df = df.sort_values([id_col, tempo_col]).reset_index(drop=True)\n",
    "    df_imputado = df.copy()\n",
    "    for var in variaveis:\n",
    "        mediana, iqr = parametros.get(var, (3600, 1800))\n",
    "        max_forward_sec = mediana + iqr\n",
    "        valor_mediano = np.nanmedian(df_imputado[var])\n",
    "        if np.isnan(valor_mediano):\n",
    "            valor_mediano = 0.0\n",
    "        for stay_id, grupo in df_imputado.groupby(id_col):\n",
    "            g = grupo.sort_values(tempo_col).reset_index()\n",
    "            tempos = g[tempo_col]\n",
    "            valores = g[var].values\n",
    "            imputed_vals = []\n",
    "            mascara = []\n",
    "            ultimo_valor = None\n",
    "            ultimo_tempo = None\n",
    "            for i in range(len(g)):\n",
    "                if not np.isnan(valores[i]):\n",
    "                    imputed_vals.append(valores[i])\n",
    "                    mascara.append(1)\n",
    "                    ultimo_valor = valores[i]\n",
    "                    ultimo_tempo = tempos.iloc[i]\n",
    "                else:\n",
    "                    if ultimo_valor is None:\n",
    "                        imputed_vals.append(normal_values.get(var, valor_mediano))\n",
    "                        mascara.append(0)\n",
    "                    else:\n",
    "                        delta = (tempos.iloc[i] - ultimo_tempo).total_seconds()\n",
    "                        if delta <= max_forward_sec:\n",
    "                            imputed_vals.append(ultimo_valor)\n",
    "                            mascara.append(0)\n",
    "                        elif delta <= 2 * max_forward_sec:\n",
    "                            frac = (delta - max_forward_sec) / max_forward_sec\n",
    "                            val = (1 - frac) * ultimo_valor + frac * valor_mediano\n",
    "                            imputed_vals.append(val)\n",
    "                            mascara.append(0)\n",
    "                        else:\n",
    "                            imputed_vals.append(valor_mediano)\n",
    "                            mascara.append(0)\n",
    "            df_imputado.loc[g['index'], var] = imputed_vals\n",
    "            df_imputado.loc[g['index'], f'{var}_mask'] = mascara\n",
    "    return df_imputado\n",
    "\n",
    "df_5min = imputacao_adaptativa(df_5min, sinais_vitais, parametros, NORMAL_VALUES)\n",
    "print(\"✅ Sinais vitais imputados com método adaptativo do HiRID e máscaras criadas.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32f0844d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51965/2046256324.py:27: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_labs = pd.read_sql(query_labs, conn)\n",
      "/home/grad/si/22/aliciachaves/miniconda3/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/grad/si/22/aliciachaves/miniconda3/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/home/grad/si/22/aliciachaves/miniconda3/lib/python3.12/site-packages/numpy/lib/_nanfunctions_impl.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exames laboratoriais imputados com interpolação, forward-fill limitado e preenchimento inicial pela mediana.\n"
     ]
    }
   ],
   "source": [
    "itemids_labs = [\n",
    "    50861, 50862, 53085, 50908, 51580, 50883, 50884, 50885, 50910, 50924,\n",
    "    50963, 50915, 52642, 51002, 51003, 50889, 52116, 51623, 50928, 52117,\n",
    "    51214, 50878, 50855, 50912, 52546, 53161, 53180, 52142, 51265, 51266,\n",
    "    52144, 50931, 50935, 51631, 51638, 51640, 51222, 51223, 50856, 51647,\n",
    "    50852, 51643, 50971, 50983, 50990, 50967, 50968, 50969, 50960, 50966,\n",
    "    50970, 50975, 51099, 51006, 51274, 51275, 51292, 51290, 51291, 50963,\n",
    "    51196, 52551, 50915, 51568, 51569, 51570, 51464, 51966, 50803, 50805,\n",
    "    50808, 50809, 50813\n",
    "]\n",
    "\n",
    "query_labs = f\"\"\"\n",
    "SELECT \n",
    "    icu.stay_id,\n",
    "    le.charttime,\n",
    "    le.itemid,\n",
    "    le.valuenum\n",
    "FROM mimiciv_hosp.labevents le\n",
    "JOIN mimiciv_icu.icustays icu\n",
    "  ON le.subject_id = icu.subject_id AND le.hadm_id = icu.hadm_id\n",
    "WHERE le.itemid IN ({','.join(map(str, itemids_labs))})\n",
    "  AND icu.stay_id IN (SELECT stay_id FROM todas_utis)\n",
    "  AND le.valuenum IS NOT NULL\n",
    "ORDER BY icu.stay_id, le.charttime;\n",
    "\"\"\"\n",
    "\n",
    "df_labs = pd.read_sql(query_labs, conn)\n",
    "df_labs['charttime'] = pd.to_datetime(df_labs['charttime'])\n",
    "\n",
    "df_labs_pivot = (\n",
    "    df_labs\n",
    "    .pivot_table(index=['stay_id', 'charttime'], columns='itemid', values='valuenum', aggfunc='mean')\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_labs_pivot.columns.name = None\n",
    "df_labs_pivot = df_labs_pivot.rename(columns={itemid: f\"lab_{itemid}\" for itemid in df_labs['itemid'].unique()})\n",
    "\n",
    "df_merged = pd.merge(df_5min, df_labs_pivot, how='left', on=['stay_id', 'charttime'])\n",
    "\n",
    "lab_vars = [col for col in df_merged.columns if col.startswith('lab_')]\n",
    "\n",
    "for var in lab_vars:\n",
    "    # Máscara de presença (1=original, 0=imputado)\n",
    "    df_merged[f'{var}_mask'] = (~df_merged[var].isna()).astype(int)\n",
    "\n",
    "    # Interpolação linear para preencher pequenos gaps\n",
    "    df_merged[var] = (\n",
    "        df_merged\n",
    "        .sort_values(['stay_id', 'charttime'])\n",
    "        .groupby('stay_id')[var]\n",
    "        .transform(lambda g: g.interpolate(method='linear', limit_direction='both'))\n",
    "    )\n",
    "\n",
    "    # Forward-fill limitado a 24h (288 janelas de 5 minutos)\n",
    "    df_merged[var] = (\n",
    "        df_merged\n",
    "        .sort_values(['stay_id', 'charttime'])\n",
    "        .groupby('stay_id')[var]\n",
    "        .transform(lambda g: g.ffill(limit=288))\n",
    "    )\n",
    "\n",
    "    # Preenchimento inicial e final com mediana observada da variável\n",
    "    median_val = df_merged[var].median()\n",
    "    df_merged[var] = (\n",
    "        df_merged\n",
    "        .groupby('stay_id')[var]\n",
    "        .transform(lambda g: g.fillna(median_val if not np.isnan(median_val) else 0))\n",
    "    )\n",
    "\n",
    "print(\"✅ Exames laboratoriais imputados com interpolação, forward-fill limitado e preenchimento inicial pela mediana.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3359fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51965/1154267511.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_vasoact = pd.read_sql(query_vasoact, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Merge por stay_id realizado com sucesso.\n",
      "✅ Falência circulatória marcada. Total: 7332\n"
     ]
    }
   ],
   "source": [
    "# Célula 5 - Extrair vasopressores e marcar falência (opção 2: merge por stay_id)\n",
    "\n",
    "query_vasoact = \"\"\"\n",
    "SELECT \n",
    "    stay_id,\n",
    "    starttime,\n",
    "    endtime,\n",
    "    dopamine,\n",
    "    epinephrine,\n",
    "    norepinephrine,\n",
    "    phenylephrine,\n",
    "    vasopressin,\n",
    "    dobutamine,\n",
    "    milrinone\n",
    "FROM mimiciv_derived.vasoactive_agent\n",
    "WHERE stay_id IN (SELECT stay_id FROM todas_utis);\n",
    "\"\"\"\n",
    "\n",
    "df_vasoact = pd.read_sql(query_vasoact, conn)\n",
    "df_vasoact['starttime'] = pd.to_datetime(df_vasoact['starttime'])\n",
    "df_vasoact['endtime'] = pd.to_datetime(df_vasoact['endtime'])\n",
    "\n",
    "vaso_cols = ['dopamine', 'epinephrine', 'norepinephrine', 'phenylephrine',\n",
    "             'vasopressin', 'dobutamine', 'milrinone']\n",
    "\n",
    "# Indicador de uso de qualquer vasopressor\n",
    "df_vasoact['vasopressor_ativo'] = df_vasoact[vaso_cols].notna().any(axis=1)\n",
    "\n",
    "# Garantir tipos corretos\n",
    "df_merged['stay_id'] = df_merged['stay_id'].astype(int)\n",
    "df_merged['charttime'] = pd.to_datetime(df_merged['charttime'])\n",
    "df_vasoact['stay_id'] = df_vasoact['stay_id'].astype(int)\n",
    "df_vasoact['starttime'] = pd.to_datetime(df_vasoact['starttime'])\n",
    "\n",
    "# Remover valores NaT se houver\n",
    "df_merged = df_merged.dropna(subset=['charttime'])\n",
    "df_vasoact = df_vasoact.dropna(subset=['starttime'])\n",
    "\n",
    "# Ordenar dentro dos grupos\n",
    "df_merged = df_merged.sort_values(['stay_id', 'charttime'], kind='mergesort').reset_index(drop=True)\n",
    "df_vasoact = df_vasoact.sort_values(['stay_id', 'starttime'], kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "# Lista para acumular os merges por grupo\n",
    "dfs_merged = []\n",
    "\n",
    "# Loop para merge por stay_id\n",
    "for sid in df_merged['stay_id'].unique():\n",
    "    df_merged_sid = df_merged[df_merged['stay_id'] == sid].sort_values('charttime')\n",
    "    df_vasoact_sid = df_vasoact[df_vasoact['stay_id'] == sid].sort_values('starttime')\n",
    "\n",
    "    if df_vasoact_sid.empty:\n",
    "        # Nenhum vaso para esse stay_id, apenas adiciona df_merged com NaNs\n",
    "        df_merged_sid['starttime'] = pd.NaT\n",
    "        df_merged_sid['endtime'] = pd.NaT\n",
    "        for col in vaso_cols:\n",
    "            df_merged_sid[col] = pd.NA\n",
    "        dfs_merged.append(df_merged_sid)\n",
    "        continue\n",
    "\n",
    "    merged_sid = pd.merge_asof(\n",
    "        df_merged_sid,\n",
    "        df_vasoact_sid[['stay_id', 'starttime', 'endtime'] + vaso_cols],\n",
    "        left_on='charttime',\n",
    "        right_on='starttime',\n",
    "        by='stay_id',\n",
    "        direction='backward',\n",
    "        tolerance=pd.Timedelta('2D')\n",
    "    )\n",
    "    dfs_merged.append(merged_sid)\n",
    "\n",
    "# Concatenar todos os grupos\n",
    "df_vaso_merged = pd.concat(dfs_merged, ignore_index=True)\n",
    "\n",
    "print(\"\\n✅ Merge por stay_id realizado com sucesso.\")\n",
    "\n",
    "# Marcar falência circulatória\n",
    "cond_tempo = (df_vaso_merged['charttime'] >= df_vaso_merged['starttime']) & \\\n",
    "             (df_vaso_merged['charttime'] <= df_vaso_merged['endtime'])\n",
    "cond_vaso = df_vaso_merged[vaso_cols].notna().any(axis=1)\n",
    "cond_mbp = df_vaso_merged['mbp'] < 65\n",
    "cond_lactato = df_vaso_merged.get('lab_50813', pd.Series(0)) >= 2\n",
    "\n",
    "df_vaso_merged['falencia'] = 0\n",
    "indices_falencia = df_vaso_merged.index[cond_tempo & cond_vaso & (cond_mbp | cond_lactato)]\n",
    "df_vaso_merged.loc[indices_falencia, 'falencia'] = 1\n",
    "\n",
    "print(f\"✅ Falência circulatória marcada. Total: {df_vaso_merged['falencia'].sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f82904ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features de instabilidade criadas.\n"
     ]
    }
   ],
   "source": [
    "# Célula 7 - Criar features de instabilidade\n",
    "\n",
    "def criar_features_instabilidade(df, event_col='falencia'):\n",
    "    df = df.sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "    df['estado_atual'] = df[event_col]\n",
    "\n",
    "    df['tempo_desde_ultimo_evento'] = np.nan\n",
    "\n",
    "    for stay_id, grupo in df.groupby('stay_id'):\n",
    "        indices = grupo.index\n",
    "        estados = grupo['estado_atual'].values\n",
    "        tempos = grupo['charttime'].values\n",
    "\n",
    "        last_event_time = None\n",
    "        tempo_desde = []\n",
    "\n",
    "        for i, estado in enumerate(estados):\n",
    "            if estado == 1:\n",
    "                last_event_time = tempos[i]\n",
    "                tempo_desde.append(0)\n",
    "            else:\n",
    "                if last_event_time is None:\n",
    "                    tempo_desde.append(np.nan)\n",
    "                else:\n",
    "                    delta = (tempos[i] - last_event_time).astype('timedelta64[m]').astype(float)\n",
    "                    tempo_desde.append(delta)\n",
    "\n",
    "        df.loc[indices, 'tempo_desde_ultimo_evento'] = tempo_desde\n",
    "\n",
    "    df['duracao_evento'] = df.groupby('stay_id')['estado_atual'].transform(lambda x: x.expanding().mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "df_vaso_merged = criar_features_instabilidade(df_vaso_merged)\n",
    "print(\"✅ Features de instabilidade criadas.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18e25149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Features de intensidade de medição criadas.\n"
     ]
    }
   ],
   "source": [
    "# Célula 8 - Criar features de intensidade de medição\n",
    "\n",
    "def criar_features_intensidade(df, vars_continuas):\n",
    "    df = df.sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "\n",
    "    # Dicionário para armazenar todas as colunas novas temporariamente\n",
    "    novas_colunas = {}\n",
    "\n",
    "    for var in vars_continuas:\n",
    "        mask = ~df[var].isna()\n",
    "\n",
    "        # Arrays temporários para armazenar valores antes do merge\n",
    "        tempo_desde_ultima_medicao = np.full(len(df), np.nan)\n",
    "        prop_medicoes = np.full(len(df), np.nan)\n",
    "\n",
    "        for stay_id, grupo in df.groupby('stay_id'):\n",
    "            indices = grupo.index\n",
    "            tempos = grupo['charttime'].values\n",
    "            mask_var = mask.loc[indices].values\n",
    "\n",
    "            last_meas_time = None\n",
    "            tempos_desde = []\n",
    "            contagem = 0\n",
    "\n",
    "            for i, presente in enumerate(mask_var):\n",
    "                if presente:\n",
    "                    last_meas_time = tempos[i]\n",
    "                    contagem += 1\n",
    "                    tempos_desde.append(0)\n",
    "                else:\n",
    "                    if last_meas_time is None:\n",
    "                        tempos_desde.append(np.nan)\n",
    "                    else:\n",
    "                        delta = (tempos[i] - last_meas_time).astype('timedelta64[m]').astype(float)\n",
    "                        tempos_desde.append(delta)\n",
    "\n",
    "            prop_medicoes_grupo = [contagem / (i+1) for i in range(len(tempos))]\n",
    "\n",
    "            tempo_desde_ultima_medicao[indices] = tempos_desde\n",
    "            prop_medicoes[indices] = prop_medicoes_grupo\n",
    "\n",
    "        novas_colunas[f'{var}_tempo_desde_ultima_medicao'] = tempo_desde_ultima_medicao\n",
    "        novas_colunas[f'{var}_prop_medicoes'] = prop_medicoes\n",
    "\n",
    "    # Criar DataFrame das novas colunas e concatenar tudo de uma vez\n",
    "    df_novas = pd.DataFrame(novas_colunas, index=df.index)\n",
    "    df = pd.concat([df, df_novas], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "vars_continuas = [col for col in df_vaso_merged.columns if col not in ['stay_id', 'charttime', 'falencia'] and not col.endswith('_mask')]\n",
    "df_vaso_merged = criar_features_intensidade(df_vaso_merged, vars_continuas)\n",
    "print(\"✅ Features de intensidade de medição criadas.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88a60a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset filtrado com 46 variáveis contínuas válidas (<50% missing).\n"
     ]
    }
   ],
   "source": [
    "vars_continuas = sinais_vitais + lab_vars  # lista completa dos seus sinais vitais e labs\n",
    "\n",
    "vars_validas = [var for var in vars_continuas if df_vaso_merged[var].isna().mean() < 0.5]\n",
    "masks_validas = [v + '_mask' for v in vars_validas]\n",
    "\n",
    "colunas_finais = vars_validas + masks_validas + ['stay_id', 'charttime', 'falencia']\n",
    "\n",
    "df_final = df_vaso_merged[colunas_finais].copy()\n",
    "\n",
    "print(f\"✅ Dataset filtrado com {len(vars_validas)} variáveis contínuas válidas (<50% missing).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed7a5439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analisando stay_id 30121250 com 997 registros\n",
      "stay_id 30121250 tem 950 janelas válidas\n",
      "\n",
      "Analisando stay_id 30155942 com 177 registros\n",
      "stay_id 30155942 tem 130 janelas válidas\n",
      "\n",
      "Analisando stay_id 30713595 com 1860 registros\n",
      "stay_id 30713595 tem 1813 janelas válidas\n",
      "\n",
      "Analisando stay_id 30974510 com 1154 registros\n",
      "stay_id 30974510 tem 1107 janelas válidas\n",
      "\n",
      "Analisando stay_id 31038553 com 1021 registros\n",
      "stay_id 31038553 tem 974 janelas válidas\n",
      "\n",
      "Analisando stay_id 31059962 com 901 registros\n",
      "stay_id 31059962 tem 854 janelas válidas\n",
      "\n",
      "Analisando stay_id 31646998 com 334 registros\n",
      "stay_id 31646998 tem 287 janelas válidas\n",
      "\n",
      "Analisando stay_id 31754815 com 3469 registros\n",
      "stay_id 31754815 tem 3422 janelas válidas\n",
      "\n",
      "Analisando stay_id 31904119 com 561 registros\n",
      "stay_id 31904119 tem 514 janelas válidas\n",
      "\n",
      "Analisando stay_id 32032568 com 178 registros\n",
      "stay_id 32032568 tem 131 janelas válidas\n",
      "\n",
      "Analisando stay_id 32280626 com 1552 registros\n",
      "stay_id 32280626 tem 1505 janelas válidas\n",
      "\n",
      "Analisando stay_id 32432981 com 488 registros\n",
      "stay_id 32432981 tem 441 janelas válidas\n",
      "\n",
      "Analisando stay_id 32458586 com 5952 registros\n",
      "stay_id 32458586 tem 5905 janelas válidas\n",
      "\n",
      "Analisando stay_id 32634741 com 1360 registros\n",
      "stay_id 32634741 tem 1313 janelas válidas\n",
      "\n",
      "Analisando stay_id 32795311 com 1141 registros\n",
      "stay_id 32795311 tem 1094 janelas válidas\n",
      "\n",
      "Analisando stay_id 32935971 com 877 registros\n",
      "stay_id 32935971 tem 830 janelas válidas\n",
      "\n",
      "Analisando stay_id 33105393 com 8834 registros\n",
      "stay_id 33105393 tem 8787 janelas válidas\n",
      "\n",
      "Analisando stay_id 33258084 com 408 registros\n",
      "stay_id 33258084 tem 361 janelas válidas\n",
      "\n",
      "Analisando stay_id 33518358 com 1187 registros\n",
      "stay_id 33518358 tem 1140 janelas válidas\n",
      "\n",
      "Analisando stay_id 33990860 com 4080 registros\n",
      "stay_id 33990860 tem 4033 janelas válidas\n",
      "\n",
      "Analisando stay_id 34153457 com 320 registros\n",
      "stay_id 34153457 tem 273 janelas válidas\n",
      "\n",
      "Analisando stay_id 34339187 com 1175 registros\n",
      "stay_id 34339187 tem 1128 janelas válidas\n",
      "\n",
      "Analisando stay_id 34455629 com 736 registros\n",
      "stay_id 34455629 tem 689 janelas válidas\n",
      "\n",
      "Analisando stay_id 34898026 com 798 registros\n",
      "stay_id 34898026 tem 751 janelas válidas\n",
      "\n",
      "Analisando stay_id 35621403 com 719 registros\n",
      "stay_id 35621403 tem 672 janelas válidas\n",
      "\n",
      "Analisando stay_id 35722221 com 1977 registros\n",
      "stay_id 35722221 tem 1930 janelas válidas\n",
      "\n",
      "Analisando stay_id 36006309 com 222 registros\n",
      "stay_id 36006309 tem 175 janelas válidas\n",
      "\n",
      "Analisando stay_id 36134276 com 408 registros\n",
      "stay_id 36134276 tem 361 janelas válidas\n",
      "\n",
      "Analisando stay_id 36160639 com 401 registros\n",
      "stay_id 36160639 tem 354 janelas válidas\n",
      "\n",
      "Analisando stay_id 36237226 com 72 registros\n",
      "stay_id 36237226 tem 25 janelas válidas\n",
      "\n",
      "Analisando stay_id 36265318 com 304 registros\n",
      "stay_id 36265318 tem 257 janelas válidas\n",
      "\n",
      "Analisando stay_id 36297377 com 661 registros\n",
      "stay_id 36297377 tem 614 janelas válidas\n",
      "\n",
      "Analisando stay_id 36415879 com 2075 registros\n",
      "stay_id 36415879 tem 2028 janelas válidas\n",
      "\n",
      "Analisando stay_id 36807273 com 5424 registros\n",
      "stay_id 36807273 tem 5377 janelas válidas\n",
      "\n",
      "Analisando stay_id 36842861 com 310 registros\n",
      "stay_id 36842861 tem 263 janelas válidas\n",
      "\n",
      "Analisando stay_id 36883287 com 781 registros\n",
      "stay_id 36883287 tem 734 janelas válidas\n",
      "\n",
      "Analisando stay_id 37088934 com 3302 registros\n",
      "stay_id 37088934 tem 3255 janelas válidas\n",
      "\n",
      "Analisando stay_id 37357381 com 2389 registros\n",
      "stay_id 37357381 tem 2342 janelas válidas\n",
      "\n",
      "Analisando stay_id 37532286 com 1406 registros\n",
      "stay_id 37532286 tem 1359 janelas válidas\n",
      "\n",
      "Analisando stay_id 37679753 com 814 registros\n",
      "stay_id 37679753 tem 767 janelas válidas\n",
      "\n",
      "Analisando stay_id 37706885 com 966 registros\n",
      "stay_id 37706885 tem 919 janelas válidas\n",
      "\n",
      "Analisando stay_id 38450462 com 2563 registros\n",
      "stay_id 38450462 tem 2516 janelas válidas\n",
      "\n",
      "Analisando stay_id 38541475 com 310 registros\n",
      "stay_id 38541475 tem 263 janelas válidas\n",
      "\n",
      "Analisando stay_id 38560644 com 861 registros\n",
      "stay_id 38560644 tem 814 janelas válidas\n",
      "\n",
      "Analisando stay_id 38947124 com 989 registros\n",
      "stay_id 38947124 tem 942 janelas válidas\n",
      "\n",
      "Analisando stay_id 39144938 com 805 registros\n",
      "stay_id 39144938 tem 758 janelas válidas\n",
      "\n",
      "Analisando stay_id 39651992 com 4249 registros\n",
      "stay_id 39651992 tem 4202 janelas válidas\n",
      "\n",
      "Analisando stay_id 39735470 com 5141 registros\n",
      "stay_id 39735470 tem 5094 janelas válidas\n",
      "\n",
      "Analisando stay_id 39793803 com 3649 registros\n",
      "stay_id 39793803 tem 3602 janelas válidas\n",
      "\n",
      "Analisando stay_id 39934059 com 311 registros\n",
      "stay_id 39934059 tem 264 janelas válidas\n",
      "\n",
      "Total janelas avaliadas: 78319\n",
      "Janelas rejeitadas por NaN: 0 (0.00%)\n",
      "\n",
      "✅ Janelas temporais criadas. Total: 78319, Formato X: (78319, 36, 172), y: (78319,)\n"
     ]
    }
   ],
   "source": [
    "def construir_janelas_temporais(df, jan_obs=36, jan_pred=12, passo=1, max_nan_ratio=0.5):\n",
    "    # Seleciona colunas ignorando ids, tempo, label e máscaras\n",
    "    candidate_cols = [col for col in df.columns if col not in ['stay_id', 'charttime', 'falencia'] and not col.endswith('_mask')]\n",
    "    \n",
    "    # Filtra somente colunas numéricas para evitar problemas com NaT ou objetos\n",
    "    vars_features = [col for col in candidate_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    if len(vars_features) == 0:\n",
    "        raise ValueError(\"Nenhuma variável numérica válida para construir janelas.\")\n",
    "    \n",
    "    X, y, stays, times = [], [], [], []\n",
    "\n",
    "    total_janelas = 0\n",
    "    rejeitadas_nan = 0\n",
    "\n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.reset_index(drop=True)\n",
    "        count_validas = 0\n",
    "        print(f\"\\nAnalisando stay_id {stay_id} com {len(group)} registros\")\n",
    "\n",
    "        for i in range(0, len(group) - (jan_obs + jan_pred) + 1, passo):\n",
    "            janela_obs = group.iloc[i : i + jan_obs]\n",
    "            janela_pred = group.iloc[i + jan_obs : i + jan_obs + jan_pred]\n",
    "\n",
    "            nan_ratio = janela_obs[vars_features].isna().mean().mean()\n",
    "            total_janelas += 1\n",
    "\n",
    "            if nan_ratio > max_nan_ratio:\n",
    "                rejeitadas_nan += 1\n",
    "                print(f\"Janela {i} rejeitada por NaN ({nan_ratio:.2%} > {max_nan_ratio:.2%})\")\n",
    "                continue\n",
    "\n",
    "            X.append(janela_obs[vars_features].values)\n",
    "            y.append(1 if janela_pred['falencia'].any() else 0)\n",
    "            stays.append(stay_id)\n",
    "            times.append(janela_obs['charttime'].iloc[0])\n",
    "            count_validas += 1\n",
    "\n",
    "        print(f\"stay_id {stay_id} tem {count_validas} janelas válidas\")\n",
    "\n",
    "    print(f\"\\nTotal janelas avaliadas: {total_janelas}\")\n",
    "    print(f\"Janelas rejeitadas por NaN: {rejeitadas_nan} ({rejeitadas_nan/total_janelas:.2%})\")\n",
    "\n",
    "    return np.array(X), np.array(y), stays, times\n",
    "\n",
    "\n",
    "# Teste\n",
    "X, y, stays, times = construir_janelas_temporais(df_vaso_merged, max_nan_ratio=0.5)\n",
    "print(f\"\\n✅ Janelas temporais criadas. Total: {len(X)}, Formato X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b78a2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Padronização z-score aplicada.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "X_shape = X.shape  # (amostras, janela_obs, features)\n",
    "X_flat = X.reshape(-1, X_shape[2])  # achata para (amostras * janela_obs, features)\n",
    "\n",
    "# Tratar NaNs e infinitos\n",
    "X_flat = np.nan_to_num(X_flat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_flat = X_flat.astype(np.float64)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_flat).reshape(X_shape)\n",
    "\n",
    "X = X_scaled\n",
    "\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "print(\"✅ Padronização z-score aplicada.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "088a087b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dados exportados.\n"
     ]
    }
   ],
   "source": [
    "# Célula 11 - Exportar dados para arquivos (opcional)\n",
    "\n",
    "np.save(\"X.npy\", X)\n",
    "np.save(\"y.npy\", y)\n",
    "pd.DataFrame({\"stay_id\": stays, \"start_time\": times}).to_csv(\"janelas_metadata.csv\", index=False)\n",
    "print(\"✅ Dados exportados.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WSL)",
   "language": "python",
   "name": "wsl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
