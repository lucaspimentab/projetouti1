{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520a08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexão estabelecida.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  database \"mimiciv\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.35, but the operating system provides version 2.39.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE mimiciv REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n"
     ]
    }
   ],
   "source": [
    "# Célula 1 - Importações e conexão ao banco\n",
    "\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mimiciv\",\n",
    "    user=\"uti_user\",\n",
    "    password=\"s0f4C1#4\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "print(\"✅ Conexão estabelecida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c20514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total admissões na UTI (>1h): 73073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87258/4039788039.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_total = pd.read_sql(\"SELECT COUNT(*) AS total_admissoes FROM todas_utis;\", conn)\n"
     ]
    }
   ],
   "source": [
    "# Célula 2 - Criar coorte UTI > 1h\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"ROLLBACK\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TEMP TABLE todas_utis AS\n",
    "SELECT \n",
    "    i.subject_id,\n",
    "    i.hadm_id,\n",
    "    i.stay_id,\n",
    "    i.intime,\n",
    "    i.outtime,\n",
    "    EXTRACT(EPOCH FROM (i.outtime - i.intime)) / 60 AS duracao_minutos\n",
    "FROM mimiciv_icu.icustays i\n",
    "WHERE EXTRACT(EPOCH FROM (i.outtime - i.intime)) > 3600;\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "df_total = pd.read_sql(\"SELECT COUNT(*) AS total_admissoes FROM todas_utis;\", conn)\n",
    "print(f\"✅ Total admissões na UTI (>1h): {df_total['total_admissoes'][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_87258/2594091736.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_vitals = pd.read_sql(query_vitals, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando consulta dos sinais vitais...\n",
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n",
      "Consulta concluída. Linhas lidas: 9745128\n",
      "Conversão charttime concluída. Exemplo:\n",
      "    stay_id           charttime\n",
      "0  30000153 2174-09-29 12:00:00\n",
      "1  30000153 2174-09-29 12:05:00\n",
      "2  30000153 2174-09-29 12:06:00\n",
      "3  30000153 2174-09-29 12:08:00\n",
      "4  30000153 2174-09-29 12:09:00\n",
      "Ordenação concluída.\n",
      "Reamostragem concluída. Tamanho: 73483564\n",
      "Exemplo após reamostragem:\n",
      "            charttime     stay_id  heart_rate    sbp   dbp   mbp  resp_rate  \\\n",
      "0 2174-09-29 12:00:00  30000153.0         NaN    NaN   NaN   NaN       14.0   \n",
      "1 2174-09-29 12:05:00  30000153.0       100.0  136.0  74.0  89.0       18.0   \n",
      "2 2174-09-29 12:10:00         NaN         NaN    NaN   NaN   NaN        NaN   \n",
      "3 2174-09-29 12:15:00         NaN         NaN    NaN   NaN   NaN        NaN   \n",
      "4 2174-09-29 12:20:00         NaN         NaN    NaN   NaN   NaN        NaN   \n",
      "\n",
      "   temperature   spo2  glucose  \n",
      "0          NaN    NaN      NaN  \n",
      "1         36.0  100.0      NaN  \n",
      "2          NaN    NaN      NaN  \n",
      "3          NaN    NaN      NaN  \n",
      "4          NaN    NaN      NaN  \n",
      "Preenchimento de stay_id concluído. Exemplo:\n",
      "      stay_id           charttime\n",
      "0  30000153.0 2174-09-29 12:00:00\n",
      "1  30000153.0 2174-09-29 12:05:00\n",
      "2  30000153.0 2174-09-29 12:10:00\n",
      "3  30000153.0 2174-09-29 12:15:00\n",
      "4  30000153.0 2174-09-29 12:20:00\n",
      "Total pacientes para imputar: 73068\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "query_vitals = \"\"\"\n",
    "SELECT\n",
    "    vs.stay_id,\n",
    "    vs.charttime,\n",
    "    vs.heart_rate,\n",
    "    vs.sbp,\n",
    "    vs.dbp,\n",
    "    vs.mbp,\n",
    "    vs.resp_rate,\n",
    "    vs.temperature,\n",
    "    vs.spo2,\n",
    "    vs.glucose\n",
    "FROM mimiciv_derived.vitalsign vs\n",
    "WHERE vs.stay_id IN (SELECT stay_id FROM todas_utis)\n",
    "ORDER BY vs.stay_id, vs.charttime;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Iniciando consulta dos sinais vitais...\")\n",
    "df_vitals = pd.read_sql(query_vitals, conn)\n",
    "print(f\"Consulta concluída. Linhas lidas: {len(df_vitals)}\")\n",
    "\n",
    "df_vitals['charttime'] = pd.to_datetime(df_vitals['charttime'])\n",
    "print(\"Conversão charttime concluída. Exemplo:\")\n",
    "print(df_vitals[['stay_id', 'charttime']].head())\n",
    "\n",
    "df_vitals = df_vitals.sort_values(['stay_id', 'charttime'])\n",
    "print(\"Ordenação concluída.\")\n",
    "\n",
    "# Reamostragem\n",
    "df_5min = (\n",
    "    df_vitals\n",
    "    .set_index('charttime')\n",
    "    .groupby('stay_id', group_keys=False)\n",
    "    .resample('5min')\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "print(f\"Reamostragem concluída. Tamanho: {len(df_5min)}\")\n",
    "print(\"Exemplo após reamostragem:\")\n",
    "print(df_5min.head())\n",
    "\n",
    "df_5min['stay_id'] = df_5min['stay_id'].ffill()\n",
    "print(\"Preenchimento de stay_id concluído. Exemplo:\")\n",
    "print(df_5min[['stay_id', 'charttime']].head())\n",
    "\n",
    "# Valores normais usados para imputação\n",
    "NORMAL_VALUES = {\n",
    "    'heart_rate': 80,\n",
    "    'sbp': 120,\n",
    "    'dbp': 80,\n",
    "    'mbp': 90,\n",
    "    'resp_rate': 16,\n",
    "    'temperature': 37,\n",
    "    'spo2': 98,\n",
    "    'glucose': 100,\n",
    "}\n",
    "\n",
    "# Função exata do artigo para imputação forward fill simples\n",
    "def impute_forward_fill_simple(raw_ts, raw_values, timegrid_pred, global_mean, fill_interval_secs=24*3600):\n",
    "    pred_values = np.zeros_like(timegrid_pred, dtype=float)\n",
    "    cum_count_ms = np.zeros_like(timegrid_pred, dtype=int)\n",
    "    time_to_last_ms = np.full(timegrid_pred.shape, -1.0, dtype=float)\n",
    "    \n",
    "    input_ts = 0\n",
    "    cum_real_meas = 0\n",
    "    last_real_ms = None\n",
    "    \n",
    "    for idx, ts in enumerate(timegrid_pred):\n",
    "        while input_ts < len(raw_ts) and raw_ts[input_ts] <= ts:\n",
    "            cum_real_meas += 1\n",
    "            last_real_ms = input_ts\n",
    "            input_ts += 1\n",
    "        \n",
    "        if input_ts == 0:\n",
    "            pred_values[idx] = global_mean\n",
    "            continue\n",
    "        \n",
    "        ext_offset = ts - raw_ts[input_ts - 1]\n",
    "        real_offset = (ts - raw_ts[last_real_ms]) if last_real_ms is not None else -1.0\n",
    "        \n",
    "        if ext_offset > fill_interval_secs:\n",
    "            pred_values[idx] = global_mean\n",
    "        else:\n",
    "            if input_ts > 1 and raw_ts[input_ts - 1] == raw_ts[input_ts - 2]:\n",
    "                pred_values[idx] = np.mean(raw_values[input_ts - 2:input_ts])\n",
    "            else:\n",
    "                pred_values[idx] = raw_values[input_ts - 1]\n",
    "        \n",
    "        cum_count_ms[idx] = cum_real_meas\n",
    "        time_to_last_ms[idx] = real_offset\n",
    "    \n",
    "    return pred_values, cum_count_ms, time_to_last_ms\n",
    "\n",
    "# Função para imputar todas as variáveis e todos os pacientes\n",
    "def imputar_df_forward_fill(df, variaveis, normal_values, tempo_col='charttime', id_col='stay_id'):\n",
    "    fill_interval_secs = 24 * 3600  # 24 horas fixo\n",
    "    df_imputado_list = []\n",
    "\n",
    "    print(f\"Total pacientes para imputar: {df[id_col].nunique()}\")\n",
    "\n",
    "    for stay_id, grupo in df.groupby(id_col):\n",
    "        grupo = grupo.sort_values(tempo_col).reset_index(drop=True)\n",
    "\n",
    "        ts_raw = grupo[tempo_col].values.astype('datetime64[s]').astype(int)\n",
    "        t0, tmax = ts_raw[0], ts_raw[-1]\n",
    "        timegrid = np.arange(t0, tmax + 300, 300)  # 5 minutos em segundos\n",
    "\n",
    "        dict_imputado = {\n",
    "            id_col: [stay_id] * len(timegrid),\n",
    "            tempo_col: pd.to_datetime(timegrid, unit='s')\n",
    "        }\n",
    "\n",
    "        for var in variaveis:\n",
    "            raw_vals = grupo[var].values\n",
    "            pred_vals, _, _ = impute_forward_fill_simple(ts_raw, raw_vals, timegrid, normal_values.get(var, np.nan), fill_interval_secs)\n",
    "            dict_imputado[var] = pred_vals\n",
    "\n",
    "            # Máscara: 1 onde foi observado dado original, 0 imputado\n",
    "            mask = np.zeros(len(timegrid), dtype=int)\n",
    "            idxs = np.searchsorted(timegrid, ts_raw)\n",
    "            idxs = idxs[idxs < len(mask)]\n",
    "            mask[idxs] = 1\n",
    "            dict_imputado[f'{var}_mask'] = mask\n",
    "\n",
    "        df_imputado_list.append(pd.DataFrame(dict_imputado))\n",
    "\n",
    "    df_imputado = pd.concat(df_imputado_list, ignore_index=True)\n",
    "    print(\"Imputação completa para todos os pacientes.\")\n",
    "\n",
    "    return df_imputado\n",
    "\n",
    "sinais_vitais = ['heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate', 'temperature', 'spo2', 'glucose']\n",
    "# Aplicar imputação forward fill simples do artigo\n",
    "df_imputado = imputar_df_forward_fill(df_5min, sinais_vitais, NORMAL_VALUES)\n",
    "\n",
    "print(\"✅ Imputação forward-fill simples concluída. Exemplo de saída:\")\n",
    "print(df_imputado.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemids_labs = [\n",
    "    50861, 50862, 53085, 50908, 51580, 50883, 50884, 50885, 50910, 50924,\n",
    "    50963, 50915, 52642, 51002, 51003, 50889, 52116, 51623, 50928, 52117,\n",
    "    51214, 50878, 50855, 50912, 52546, 53161, 53180, 52142, 51265, 51266,\n",
    "    52144, 50931, 50935, 51631, 51638, 51640, 51222, 51223, 50856, 51647,\n",
    "    50852, 51643, 50971, 50983, 50990, 50967, 50968, 50969, 50960, 50966,\n",
    "    50970, 50975, 51099, 51006, 51274, 51275, 51292, 51290, 51291, 50963,\n",
    "    51196, 52551, 50915, 51568, 51569, 51570, 51464, 51966, 50803, 50805,\n",
    "    50808, 50809, 50813\n",
    "]\n",
    "\n",
    "query_labs = f\"\"\"\n",
    "SELECT \n",
    "    icu.stay_id,\n",
    "    le.charttime,\n",
    "    le.itemid,\n",
    "    le.valuenum\n",
    "FROM mimiciv_hosp.labevents le\n",
    "JOIN mimiciv_icu.icustays icu\n",
    "  ON le.subject_id = icu.subject_id AND le.hadm_id = icu.hadm_id\n",
    "WHERE le.itemid IN ({','.join(map(str, itemids_labs))})\n",
    "  AND icu.stay_id IN (SELECT stay_id FROM todas_utis)\n",
    "  AND le.valuenum IS NOT NULL\n",
    "ORDER BY icu.stay_id, le.charttime;\n",
    "\"\"\"\n",
    "\n",
    "print(\"Iniciando consulta de exames laboratoriais...\")\n",
    "df_labs = pd.read_sql(query_labs, conn)\n",
    "print(f\"Consulta concluída. Linhas lidas: {len(df_labs)}\")\n",
    "\n",
    "df_labs['charttime'] = pd.to_datetime(df_labs['charttime'])\n",
    "print(\"Conversão do charttime concluída.\")\n",
    "\n",
    "print(\"Iniciando pivot_table dos exames laboratoriais...\")\n",
    "df_labs_pivot = (\n",
    "    df_labs\n",
    "    .pivot_table(index=['stay_id', 'charttime'], columns='itemid', values='valuenum', aggfunc='mean')\n",
    "    .reset_index()\n",
    ")\n",
    "print(f\"Pivot concluído. Tamanho do DataFrame: {len(df_labs_pivot)}\")\n",
    "\n",
    "df_labs_pivot.columns.name = None\n",
    "df_labs_pivot = df_labs_pivot.rename(columns={itemid: f\"lab_{itemid}\" for itemid in df_labs['itemid'].unique()})\n",
    "print(\"Renomeação das colunas concluída.\")\n",
    "\n",
    "df_merged = pd.merge(df_5min, df_labs_pivot, how='left', on=['stay_id', 'charttime'])\n",
    "print(f\"Merge com sinais vitais concluído. Tamanho do DataFrame: {len(df_merged)}\")\n",
    "\n",
    "lab_vars = [col for col in df_merged.columns if col.startswith('lab_')]\n",
    "\n",
    "for idx, var in enumerate(lab_vars, 1):\n",
    "    print(f\"\\n[{idx}/{len(lab_vars)}] Iniciando tratamento da variável laboratorial: {var}\")\n",
    "\n",
    "    # Máscara inicial de presença (1 = valor original observado antes da imputação)\n",
    "    df_merged[f'{var}_mask'] = (~df_merged[var].isna()).astype(int)\n",
    "    print(f\"  Máscara inicial criada para {var}\")\n",
    "\n",
    "    # Calcular mediana global para imputação\n",
    "    median_val = df_merged[var].median()\n",
    "    if np.isnan(median_val):\n",
    "        median_val = 0\n",
    "    print(f\"  Mediana global calculada para {var}: {median_val}\")\n",
    "\n",
    "    # Combinar interpolação linear e forward-fill limitado em uma única transformação\n",
    "    def interp_ffill(g):\n",
    "        return g.interpolate(method='linear', limit_direction='both').ffill(limit=288)\n",
    "\n",
    "    df_merged[var] = (\n",
    "        df_merged\n",
    "        .sort_values(['stay_id', 'charttime'])\n",
    "        .groupby('stay_id')[var]\n",
    "        .transform(interp_ffill)\n",
    "    )\n",
    "    print(f\"  Interpolação + forward-fill concluídos para {var}\")\n",
    "\n",
    "    # Preencher valores faltantes restantes com a mediana global\n",
    "    df_merged[var] = df_merged[var].fillna(median_val)\n",
    "    print(f\"  Imputação final com mediana global concluída para {var}\")\n",
    "\n",
    "    # Atualizar máscara para indicar valores originais (1) e imputados (0)\n",
    "    df_merged[f'{var}_mask_imputed'] = (~df_merged[var].isna()).astype(int)\n",
    "    print(f\"  Máscara atualizada (mask_imputed) para {var}\")\n",
    "\n",
    "print(\"\\n✅ Exames laboratoriais imputados com interpolação, forward-fill limitado e preenchimento com mediana global.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3359fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5 - Extrair vasopressores e marcar falência (opção 2: merge por stay_id)\n",
    "\n",
    "query_vasoact = \"\"\"\n",
    "SELECT \n",
    "    stay_id,\n",
    "    starttime,\n",
    "    endtime,\n",
    "    dopamine,\n",
    "    epinephrine,\n",
    "    norepinephrine,\n",
    "    phenylephrine,\n",
    "    vasopressin,\n",
    "    dobutamine,\n",
    "    milrinone\n",
    "FROM mimiciv_derived.vasoactive_agent\n",
    "WHERE stay_id IN (SELECT stay_id FROM todas_utis);\n",
    "\"\"\"\n",
    "\n",
    "df_vasoact = pd.read_sql(query_vasoact, conn)\n",
    "df_vasoact['starttime'] = pd.to_datetime(df_vasoact['starttime'])\n",
    "df_vasoact['endtime'] = pd.to_datetime(df_vasoact['endtime'])\n",
    "\n",
    "vaso_cols = ['dopamine', 'epinephrine', 'norepinephrine', 'phenylephrine',\n",
    "             'vasopressin', 'dobutamine', 'milrinone']\n",
    "\n",
    "# Indicador de uso de qualquer vasopressor\n",
    "df_vasoact['vasopressor_ativo'] = df_vasoact[vaso_cols].notna().any(axis=1)\n",
    "\n",
    "# Garantir tipos corretos\n",
    "df_merged['stay_id'] = df_merged['stay_id'].astype(int)\n",
    "df_merged['charttime'] = pd.to_datetime(df_merged['charttime'])\n",
    "df_vasoact['stay_id'] = df_vasoact['stay_id'].astype(int)\n",
    "df_vasoact['starttime'] = pd.to_datetime(df_vasoact['starttime'])\n",
    "\n",
    "# Remover valores NaT se houver\n",
    "df_merged = df_merged.dropna(subset=['charttime'])\n",
    "df_vasoact = df_vasoact.dropna(subset=['starttime'])\n",
    "\n",
    "# Ordenar dentro dos grupos\n",
    "df_merged = df_merged.sort_values(['stay_id', 'charttime'], kind='mergesort').reset_index(drop=True)\n",
    "df_vasoact = df_vasoact.sort_values(['stay_id', 'starttime'], kind='mergesort').reset_index(drop=True)\n",
    "\n",
    "# Lista para acumular os merges por grupo\n",
    "dfs_merged = []\n",
    "\n",
    "# Loop para merge por stay_id\n",
    "for sid in df_merged['stay_id'].unique():\n",
    "    df_merged_sid = df_merged[df_merged['stay_id'] == sid].sort_values('charttime')\n",
    "    df_vasoact_sid = df_vasoact[df_vasoact['stay_id'] == sid].sort_values('starttime')\n",
    "\n",
    "    if df_vasoact_sid.empty:\n",
    "        # Nenhum vaso para esse stay_id, apenas adiciona df_merged com NaNs\n",
    "        df_merged_sid['starttime'] = pd.NaT\n",
    "        df_merged_sid['endtime'] = pd.NaT\n",
    "        for col in vaso_cols:\n",
    "            df_merged_sid[col] = pd.NA\n",
    "        dfs_merged.append(df_merged_sid)\n",
    "        continue\n",
    "\n",
    "    merged_sid = pd.merge_asof(\n",
    "        df_merged_sid,\n",
    "        df_vasoact_sid[['stay_id', 'starttime', 'endtime'] + vaso_cols],\n",
    "        left_on='charttime',\n",
    "        right_on='starttime',\n",
    "        by='stay_id',\n",
    "        direction='backward',\n",
    "        tolerance=pd.Timedelta('2D')\n",
    "    )\n",
    "    dfs_merged.append(merged_sid)\n",
    "\n",
    "# Concatenar todos os grupos\n",
    "df_vaso_merged = pd.concat(dfs_merged, ignore_index=True)\n",
    "\n",
    "print(\"\\n✅ Merge por stay_id realizado com sucesso.\")\n",
    "\n",
    "# Marcar falência circulatória\n",
    "cond_tempo = (df_vaso_merged['charttime'] >= df_vaso_merged['starttime']) & \\\n",
    "             (df_vaso_merged['charttime'] <= df_vaso_merged['endtime'])\n",
    "cond_vaso = df_vaso_merged[vaso_cols].notna().any(axis=1)\n",
    "cond_mbp = df_vaso_merged['mbp'] < 65\n",
    "cond_lactato = df_vaso_merged.get('lab_50813', pd.Series(0)) >= 2\n",
    "\n",
    "df_vaso_merged['falencia'] = 0\n",
    "indices_falencia = df_vaso_merged.index[cond_tempo & cond_vaso & (cond_mbp | cond_lactato)]\n",
    "df_vaso_merged.loc[indices_falencia, 'falencia'] = 1\n",
    "\n",
    "print(f\"✅ Falência circulatória marcada. Total: {df_vaso_merged['falencia'].sum()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82904ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7 - Criar features de instabilidade\n",
    "\n",
    "def criar_features_instabilidade(df, event_col='falencia'):\n",
    "    df = df.sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "    df['estado_atual'] = df[event_col]\n",
    "\n",
    "    df['tempo_desde_ultimo_evento'] = np.nan\n",
    "\n",
    "    for stay_id, grupo in df.groupby('stay_id'):\n",
    "        indices = grupo.index\n",
    "        estados = grupo['estado_atual'].values\n",
    "        tempos = grupo['charttime'].values\n",
    "\n",
    "        last_event_time = None\n",
    "        tempo_desde = []\n",
    "\n",
    "        for i, estado in enumerate(estados):\n",
    "            if estado == 1:\n",
    "                last_event_time = tempos[i]\n",
    "                tempo_desde.append(0)\n",
    "            else:\n",
    "                if last_event_time is None:\n",
    "                    tempo_desde.append(np.nan)\n",
    "                else:\n",
    "                    delta = (tempos[i] - last_event_time).astype('timedelta64[m]').astype(float)\n",
    "                    tempo_desde.append(delta)\n",
    "\n",
    "        df.loc[indices, 'tempo_desde_ultimo_evento'] = tempo_desde\n",
    "\n",
    "    df['duracao_evento'] = df.groupby('stay_id')['estado_atual'].transform(lambda x: x.expanding().mean())\n",
    "\n",
    "    return df\n",
    "\n",
    "df_vaso_merged = criar_features_instabilidade(df_vaso_merged)\n",
    "print(\"✅ Features de instabilidade criadas.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e25149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 8 - Criar features de intensidade de medição\n",
    "\n",
    "def criar_features_intensidade(df, vars_continuas):\n",
    "    df = df.sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "\n",
    "    # Dicionário para armazenar todas as colunas novas temporariamente\n",
    "    novas_colunas = {}\n",
    "\n",
    "    for var in vars_continuas:\n",
    "        mask = ~df[var].isna()\n",
    "\n",
    "        # Arrays temporários para armazenar valores antes do merge\n",
    "        tempo_desde_ultima_medicao = np.full(len(df), np.nan)\n",
    "        prop_medicoes = np.full(len(df), np.nan)\n",
    "\n",
    "        for stay_id, grupo in df.groupby('stay_id'):\n",
    "            indices = grupo.index\n",
    "            tempos = grupo['charttime'].values\n",
    "            mask_var = mask.loc[indices].values\n",
    "\n",
    "            last_meas_time = None\n",
    "            tempos_desde = []\n",
    "            contagem = 0\n",
    "\n",
    "            for i, presente in enumerate(mask_var):\n",
    "                if presente:\n",
    "                    last_meas_time = tempos[i]\n",
    "                    contagem += 1\n",
    "                    tempos_desde.append(0)\n",
    "                else:\n",
    "                    if last_meas_time is None:\n",
    "                        tempos_desde.append(np.nan)\n",
    "                    else:\n",
    "                        delta = (tempos[i] - last_meas_time).astype('timedelta64[m]').astype(float)\n",
    "                        tempos_desde.append(delta)\n",
    "\n",
    "            prop_medicoes_grupo = [contagem / (i+1) for i in range(len(tempos))]\n",
    "\n",
    "            tempo_desde_ultima_medicao[indices] = tempos_desde\n",
    "            prop_medicoes[indices] = prop_medicoes_grupo\n",
    "\n",
    "        novas_colunas[f'{var}_tempo_desde_ultima_medicao'] = tempo_desde_ultima_medicao\n",
    "        novas_colunas[f'{var}_prop_medicoes'] = prop_medicoes\n",
    "\n",
    "    # Criar DataFrame das novas colunas e concatenar tudo de uma vez\n",
    "    df_novas = pd.DataFrame(novas_colunas, index=df.index)\n",
    "    df = pd.concat([df, df_novas], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "vars_continuas = [col for col in df_vaso_merged.columns if col not in ['stay_id', 'charttime', 'falencia'] and not col.endswith('_mask')]\n",
    "df_vaso_merged = criar_features_intensidade(df_vaso_merged, vars_continuas)\n",
    "print(\"✅ Features de intensidade de medição criadas.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extrair_features_cumulativas(df, exclude_cols=None, id_col='stay_id', time_col='charttime'):\n",
    "    \"\"\"\n",
    "    Gera para cada paciente e cada variável contínua:\n",
    "    - Min cumulativo (cummin)\n",
    "    - Max cumulativo (cummax)\n",
    "    - Média cumulativa (cumsum / contagem)\n",
    "    - Número cumulativo de medições (não NaN)\n",
    "\n",
    "    Parâmetros:\n",
    "    - df: DataFrame com dados já ordenados por id_col e time_col\n",
    "    - exclude_cols: lista de colunas a excluir (ex: ['falencia', col_mask...])\n",
    "    - id_col: nome da coluna de paciente\n",
    "    - time_col: nome da coluna de tempo\n",
    "\n",
    "    Retorna:\n",
    "    - df com as novas colunas adicionadas\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "\n",
    "    # Seleciona as variáveis contínuas a processar\n",
    "    cols = [c for c in df.columns if c not in exclude_cols + [id_col, time_col] and not c.endswith('_mask')]\n",
    "\n",
    "    df = df.sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "    df_feat = df.copy()\n",
    "\n",
    "    for col in cols:\n",
    "        df_feat[f'min_{col}'] = df_feat.groupby(id_col)[col].cummin().ffill()\n",
    "        df_feat[f'max_{col}'] = df_feat.groupby(id_col)[col].cummax().ffill()\n",
    "\n",
    "        n_meas = df_feat.groupby(id_col)[col].apply(lambda x: x.notna().cumsum()).reset_index(level=0, drop=True)\n",
    "        cumsum = df_feat.groupby(id_col)[col].cumsum().ffill()\n",
    "\n",
    "        df_feat[f'n_meas_{col}'] = n_meas\n",
    "        df_feat[f'mean_{col}'] = cumsum / n_meas.replace(0, pd.NA)\n",
    "        df_feat[f'mean_{col}'] = df_feat[f'mean_{col}'].fillna(method='ffill')\n",
    "\n",
    "    return df_feat\n",
    "\n",
    "exclude = ['falencia'] + [col for col in df_vaso_merged.columns if col.endswith('_mask')]\n",
    "\n",
    "df_completo = extrair_features_cumulativas(df_vaso_merged, exclude_cols=exclude)\n",
    "\n",
    "print(\"✅ Features cumulativas extraídas.\")\n",
    "print(df_completo.filter(regex='^(min_|max_|mean_|n_meas_)').head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a60a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_continuas = sinais_vitais + lab_vars  # lista completa dos seus sinais vitais e labs\n",
    "\n",
    "vars_validas = [var for var in vars_continuas if df_vaso_merged[var].isna().mean() < 0.5]\n",
    "masks_validas = [v + '_mask' for v in vars_validas]\n",
    "\n",
    "colunas_finais = vars_validas + masks_validas + ['stay_id', 'charttime', 'falencia']\n",
    "\n",
    "df_final = df_vaso_merged[colunas_finais].copy()\n",
    "\n",
    "print(f\"✅ Dataset filtrado com {len(vars_validas)} variáveis contínuas válidas (<50% missing).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def construir_janelas_temporais(df, jan_obs=36, jan_pred=12, passo=1, max_nan_ratio=0.5):\n",
    "    # Seleciona colunas ignorando ids, tempo, label e máscaras\n",
    "    candidate_cols = [\n",
    "        col for col in df.columns \n",
    "        if col not in ['stay_id', 'charttime', 'falencia'] and not col.endswith('_mask')\n",
    "    ]\n",
    "    \n",
    "    # Filtra somente colunas numéricas para evitar problemas com NaT ou objetos\n",
    "    vars_features = [col for col in candidate_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
    "    \n",
    "    if len(vars_features) == 0:\n",
    "        raise ValueError(\"Nenhuma variável numérica válida para construir janelas.\")\n",
    "    \n",
    "    X, y, stays, times = [], [], [], []\n",
    "\n",
    "    total_janelas = 0\n",
    "    rejeitadas_nan = 0\n",
    "\n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.reset_index(drop=True)\n",
    "        count_validas = 0\n",
    "        print(f\"\\nAnalisando stay_id {stay_id} com {len(group)} registros\")\n",
    "\n",
    "        max_start = len(group) - (jan_obs + jan_pred) + 1\n",
    "        for i in range(0, max_start, passo):\n",
    "            janela_obs = group.iloc[i : i + jan_ob