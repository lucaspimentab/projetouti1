{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "520a08de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexão ao banco estabelecida.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  database \"mimiciv\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.35, but the operating system provides version 2.39.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE mimiciv REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "\n",
    "# Configurações gerais\n",
    "BATCH_SIZE = 10000\n",
    "OUTPUT_DIR_VITALS = 'output/vitals_batches'\n",
    "OUTPUT_DIR_LABS = 'output/labs_batches'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR_VITALS, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_LABS, exist_ok=True)\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mimiciv\",\n",
    "    user=\"uti_user\",\n",
    "    password=\"s0f4C1#4\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "print(\"✅ Conexão ao banco estabelecida.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28c20514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total admissões na UTI (>1h): 73073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/1640830532.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_total = pd.read_sql(\"SELECT COUNT(*) AS total_admissoes FROM todas_utis;\", conn)\n",
      "/tmp/ipykernel_102500/1640830532.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  todas_utis = pd.read_sql(\"SELECT DISTINCT stay_id FROM todas_utis\", conn)['stay_id'].tolist()\n"
     ]
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "cursor.execute(\"ROLLBACK\")\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TEMP TABLE todas_utis AS\n",
    "SELECT \n",
    "    i.subject_id,\n",
    "    i.hadm_id,\n",
    "    i.stay_id,\n",
    "    i.intime,\n",
    "    i.outtime,\n",
    "    EXTRACT(EPOCH FROM (i.outtime - i.intime)) / 60 AS duracao_minutos\n",
    "FROM mimiciv_icu.icustays i\n",
    "WHERE EXTRACT(EPOCH FROM (i.outtime - i.intime)) > 3600;\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "df_total = pd.read_sql(\"SELECT COUNT(*) AS total_admissoes FROM todas_utis;\", conn)\n",
    "print(f\"✅ Total admissões na UTI (>1h): {df_total['total_admissoes'][0]}\")\n",
    "\n",
    "todas_utis = pd.read_sql(\"SELECT DISTINCT stay_id FROM todas_utis\", conn)['stay_id'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1b4930e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alta frequência: 36 variáveis\n",
      "Média frequência: 10 variáveis\n",
      "Baixa frequência: 33 variáveis\n",
      "Farmacológicas: 7 variáveis\n"
     ]
    }
   ],
   "source": [
    "vars_alta_freq = [\n",
    "    'heart_rate', 'sbp', 'dbp', 'mbp', 'resp_rate', 'spo2',\n",
    "    'lab_53085', 'lab_51580', 'lab_52642', 'lab_51002', 'lab_52116', 'lab_51623',\n",
    "    'lab_50928', 'lab_52117', 'lab_50855', 'lab_52546', 'lab_53161', 'lab_53180',\n",
    "    'lab_52142', 'lab_51266', 'lab_52144', 'lab_51631', 'lab_51638', 'lab_51640',\n",
    "    'lab_51647', 'lab_51643', 'lab_50975', 'lab_51292', 'lab_51290', 'lab_51291',\n",
    "    'lab_52551', 'lab_51568', 'lab_51569', 'lab_51570', 'lab_51464', 'lab_51966'\n",
    "]\n",
    "\n",
    "vars_media_freq = [\n",
    "    'temperature', 'glucose', 'lab_50908', 'lab_50915', 'lab_50856', 'lab_50803',\n",
    "    'lab_50805', 'lab_50808', 'lab_50809', 'lab_50813'\n",
    "]\n",
    "\n",
    "vars_baixa_freq = [\n",
    "    'lab_50861', 'lab_50862', 'lab_50883', 'lab_50884', 'lab_50885', 'lab_50910',\n",
    "    'lab_50924', 'lab_50963', 'lab_51003', 'lab_50889', 'lab_51214', 'lab_50878',\n",
    "    'lab_50912', 'lab_51265', 'lab_50931', 'lab_50935', 'lab_51222', 'lab_51223',\n",
    "    'lab_50852', 'lab_50971', 'lab_50983', 'lab_50990', 'lab_50967', 'lab_50968',\n",
    "    'lab_50969', 'lab_50960', 'lab_50966', 'lab_50970', 'lab_51099', 'lab_51006',\n",
    "    'lab_51274', 'lab_51275', 'lab_51196'\n",
    "]\n",
    "\n",
    "vasopressor_vars = [\n",
    "    'dopamine', 'epinephrine', 'norepinephrine', 'phenylephrine',\n",
    "    'vasopressin', 'dobutamine', 'milrinone'\n",
    "]\n",
    "\n",
    "print(f\"Alta frequência: {len(vars_alta_freq)} variáveis\")\n",
    "print(f\"Média frequência: {len(vars_media_freq)} variáveis\")\n",
    "print(f\"Baixa frequência: {len(vars_baixa_freq)} variáveis\")\n",
    "print(f\"Farmacológicas: {len(vasopressor_vars)} variáveis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84eb88d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Valores normais e parâmetros de imputação configurados.\n"
     ]
    }
   ],
   "source": [
    "# Valores normais (default values) para variáveis (conforme artigo/excel)\n",
    "NORMAL_VALUES = {\n",
    "    'heart_rate': 70,\n",
    "    'sbp': 125,\n",
    "    'dbp': 75,\n",
    "    'mbp': 90,\n",
    "    'resp_rate': 12,\n",
    "    'temperature': 37,\n",
    "    'spo2': 98,\n",
    "    'glucose': 5,  # mmol/L, conforme tabela (se precisar converter, ajuste)\n",
    "\n",
    "}\n",
    "\n",
    "for var in vasopressor_vars:\n",
    "    NORMAL_VALUES[var] = 0\n",
    "\n",
    "# Parâmetros mediana e IQR obtidos do cálculo com amostra (ou valores do artigo)\n",
    "PARAMETROS_IMPUTACAO = {\n",
    "    'lab_50861': {'default': 1, 'range': (0, 15)},               # Lactate, Blood\n",
    "    'lab_50862': {'default': 1, 'range': (0, 15)},               # Lactate, Arterial\n",
    "    'lab_53085': {'default': 1, 'range': (0, 15)},               # Lactate, Venous\n",
    "    'lab_50908': {'default': 1, 'range': (0, 8)},                # INR (PT)\n",
    "    'lab_51580': {'default': 1, 'range': (0, 8)},                # INR (PT), Arterial\n",
    "    'lab_50963': {'default': 4, 'range': (0, 600)},              # C-Reactive Protein\n",
    "    'lab_50889': {'default': 4, 'range': (2, 12)},               # Potassium, Serum or Plasma\n",
    "    'lab_52116': {'default': 4, 'range': (2, 12)},               # Potassium, Blood\n",
    "    'lab_51623': {'default': 140, 'range': (105, 170)},          # Sodium, Serum or Plasma\n",
    "    'lab_50928': {'default': 140, 'range': (105, 170)},          # Sodium, Blood\n",
    "    'lab_52117': {'default': 140, 'range': (105, 170)},          # Sodium, Blood\n",
    "    'lab_51214': {'default': 24, 'range': (0, 50)},              # Bicarbonate, Serum\n",
    "    'lab_50878': {'default': 103, 'range': (60, 140)},           # Chloride, Serum or Plasma\n",
    "    'lab_50855': {'default': 1, 'range': (0, 3)},                # Calcium, Serum\n",
    "    'lab_50912': {'default': 1, 'range': (0, 5)},                # Magnesium, Serum or Plasma\n",
    "    'lab_52546': {'default': 1, 'range': (0, 5)},                # Phosphate, Serum\n",
    "    'lab_53161': {'default': 1, 'range': (0, 5)},                # Phosphate, Blood\n",
    "    'lab_53180': {'default': 1, 'range': (0, 5)},                # Phosphate, Urine\n",
    "    'lab_52142': {'default': 2, 'range': (0, 1000)},             # Bilirubin, Total Serum\n",
    "    'lab_51265': {'default': 2, 'range': (0, 800)},              # Bilirubin, Direct Serum\n",
    "    'lab_52144': {'default': 2, 'range': (0, 1000)},             # Bilirubin, Blood (assumido igual Total)\n",
    "    'lab_50931': {'default': 38, 'range': (0, 70)},              # Albumin, Serum or Plasma\n",
    "    'lab_51631': {'default': 85, 'range': (0, 3000)},            # Alkaline Phosphatase, Serum\n",
    "    'lab_51638': {'default': 25, 'range': (0, 12000)},           # Alanine Aminotransferase (ALT), Serum\n",
    "    'lab_51640': {'default': 25, 'range': (0, 30000)},           # Aspartate Aminotransferase (AST), Serum\n",
    "    'lab_51223': {'default': 50, 'range': (0, 30000)},           # Creatine Kinase, Serum\n",
    "    'lab_50856': {'default': 5, 'range': (0, 60)},               # Urea Nitrogen (BUN), Serum\n",
    "    'lab_51647': {'default': 200, 'range': (0, 15000)},          # Troponin T, Serum (valor estimado)\n",
    "    'lab_50852': {'default': 200, 'range': (0, 15000)},          # Troponin I, Serum (valor estimado)\n",
    "    'lab_50983': {'default': 135, 'range': (20, 200)},           # Hemoglobin, Blood\n",
    "    'lab_50968': {'default': 300, 'range': (0, 1500)},           # Platelet Count, Blood (estimado)\n",
    "    'lab_50960': {'default': 99, 'range': (60, 130)},            # Mean Corpuscular Volume (MCV), Blood\n",
    "    'lab_51196': {'default': 7, 'range': (6.5, 7.8)},             # Blood Gas, pH\n",
    "    'lab_52551': {'default': 40, 'range': (3, 100)},              # Blood Gas, pCO2\n",
    "    'lab_51568': {'default': 87, 'range': (20, 500)},             # Blood Gas, pO2\n",
    "    'lab_51569': {'default': 96, 'range': (10, 100)},             # Blood Gas, Oxygen Saturation\n",
    "    'lab_51570': {'default': 24, 'range': (0, 50)},               # Blood Gas, Bicarbonate\n",
    "    'lab_51966': {'default': 1, 'range': (0, 15)},                # Blood Gas, Lactate\n",
    "    'lab_50803': {'default': 140, 'range': (105, 170)},           # Blood Gas, Sodium\n",
    "    'lab_50805': {'default': 4, 'range': (2, 12)},                # Blood Gas, Potassium\n",
    "    'lab_50808': {'default': 1, 'range': (0, 3)},                 # Blood Gas, Calcium\n",
    "    'lab_50809': {'default': 135, 'range': (20, 200)},            # Blood Gas, Hemoglobin\n",
    "    'lab_50813': {'default': 45, 'range': (30, 60)},              # Blood Gas, Hematocrit (aproximado)\n",
    "}\n",
    "\n",
    "\n",
    "print(\"✅ Valores normais e parâmetros de imputação configurados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32f0844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemids_labs = [\n",
    "    50861, 50862, 53085, 50908, 51580, 50883, 50884, 50885, 50910, 50924,\n",
    "    50963, 50915, 52642, 51002, 51003, 50889, 52116, 51623, 50928, 52117,\n",
    "    51214, 50878, 50855, 50912, 52546, 53161, 53180, 52142, 51265, 51266,\n",
    "    52144, 50931, 50935, 51631, 51638, 51640, 51222, 51223, 50856, 51647,\n",
    "    50852, 51643, 50971, 50983, 50990, 50967, 50968, 50969, 50960, 50966,\n",
    "    50970, 50975, 51099, 51006, 51274, 51275, 51292, 51290, 51291, 50963,\n",
    "    51196, 52551, 50915, 51568, 51569, 51570, 51464, 51966, 50803, 50805,\n",
    "    50808, 50809, 50813\n",
    "]\n",
    "\n",
    "def value_empty(size, default_val, dtype=None):\n",
    "    \"\"\" Retorna vetor preenchido com valor default \"\"\"\n",
    "    if dtype is not None:\n",
    "        tmp_arr = np.empty(size, dtype=dtype)\n",
    "    else:\n",
    "        tmp_arr = np.empty(size)\n",
    "    tmp_arr[:] = default_val\n",
    "    return tmp_arr\n",
    "\n",
    "def empty_nan(size):\n",
    "    \"\"\" Retorna vetor preenchido com NaN \"\"\"\n",
    "    arr = np.empty(size)\n",
    "    arr[:] = np.nan\n",
    "    return arr\n",
    "\n",
    "def impute_forward_fill_simple(observ_ts, observ_val, time_grid, global_impute_val=np.nan):\n",
    "    \"\"\"\n",
    "    Forward-fill simples para preenchimento no grid uniforme de tempo.\n",
    "    Se não houver dado anterior, usa global_impute_val (NaN para sinais vitais normais, 0 para farmacológicas).\n",
    "    \"\"\"\n",
    "    n = len(time_grid)\n",
    "    pred_values = np.full(n, global_impute_val, dtype=np.float32)\n",
    "    last_val = global_impute_val\n",
    "    i_obs = 0\n",
    "\n",
    "    for i_pred, t_pred in enumerate(time_grid):\n",
    "        # Avança no vetor de observações enquanto timestamps <= timestamp da previsão\n",
    "        while i_obs < len(observ_ts) and observ_ts[i_obs] <= t_pred:\n",
    "            last_val = observ_val[i_obs]\n",
    "            i_obs += 1\n",
    "\n",
    "        pred_values[i_pred] = last_val\n",
    "\n",
    "    return pred_values\n",
    "\n",
    "def imputar_batch_simples(df_batch, variaveis, global_impute_vals):\n",
    "    df_imputado_list = []\n",
    "\n",
    "    for stay_id, grupo in df_batch.groupby('stay_id'):\n",
    "        grupo = grupo.sort_values('charttime').reset_index(drop=True)\n",
    "        ts_raw = grupo['charttime'].values.astype('datetime64[s]').astype(np.int64)\n",
    "        t0, tmax = ts_raw[0], ts_raw[-1]\n",
    "        timegrid = np.arange(t0, tmax + 300, 300)  # passo de 5 minutos\n",
    "\n",
    "        dict_imputado = {\n",
    "            'stay_id': [stay_id] * len(timegrid),\n",
    "            'charttime': pd.to_datetime(timegrid, unit='s')\n",
    "        }\n",
    "\n",
    "        for var in variaveis:\n",
    "            if var not in grupo.columns or grupo[var].dropna().empty:\n",
    "                # Nenhum dado observado: preenche com valor global_impute_val\n",
    "                fill_val = global_impute_vals.get(var, np.nan)\n",
    "                dict_imputado[var] = value_empty(len(timegrid), fill_val)\n",
    "                dict_imputado[f'{var}_imputed'] = np.ones(len(timegrid), dtype=int)  # 1 indica imputado\n",
    "                continue\n",
    "\n",
    "            raw_vals = grupo[var].values\n",
    "            pred_vals = impute_forward_fill_simple(ts_raw, raw_vals, timegrid, global_impute_vals.get(var, np.nan))\n",
    "            dict_imputado[var] = pred_vals\n",
    "\n",
    "            # Máscara: 0 para imputado, 1 para observado\n",
    "            mask = np.zeros(len(timegrid), dtype=int)\n",
    "            idxs = np.searchsorted(timegrid, ts_raw)\n",
    "            idxs = idxs[idxs < len(mask)]\n",
    "            mask[idxs] = 1\n",
    "            dict_imputado[f'{var}_imputed'] = 1 - mask  # 1 se imputado, 0 se observado\n",
    "\n",
    "        df_imputado_list.append(pd.DataFrame(dict_imputado))\n",
    "\n",
    "    return pd.concat(df_imputado_list, ignore_index=True)\n",
    "\n",
    "# Exemplo de uso para um batch (conectado a banco e dataframe já definidos)\n",
    "def processar_batch_vitals(batch_stay_ids):\n",
    "    batch_ids_str = \",\".join(map(str, batch_stay_ids))\n",
    "    query_vitals = f\"\"\"\n",
    "    SELECT\n",
    "        vs.stay_id,\n",
    "        vs.charttime,\n",
    "        vs.heart_rate,\n",
    "        vs.sbp,\n",
    "        vs.dbp,\n",
    "        vs.mbp,\n",
    "        vs.resp_rate,\n",
    "        vs.temperature,\n",
    "        vs.spo2,\n",
    "        vs.glucose,\n",
    "        vs.dopamine,\n",
    "        vs.epinephrine,\n",
    "        vs.norepinephrine,\n",
    "        vs.phenylephrine,\n",
    "        vs.vasopressin,\n",
    "        vs.dobutamine,\n",
    "        vs.milrinone\n",
    "    FROM mimiciv_derived.vitalsign vs\n",
    "    WHERE vs.stay_id IN ({batch_ids_str})\n",
    "    ORDER BY vs.stay_id, vs.charttime;\n",
    "    \"\"\"\n",
    "    print(f\"Consultando sinais vitais batch com {len(batch_stay_ids)} stays...\")\n",
    "    df_batch = pd.read_sql(query_vitals, conn)\n",
    "    df_batch['charttime'] = pd.to_datetime(df_batch['charttime'])\n",
    "    print(f\"Consulta concluída. Linhas: {len(df_batch)}\")\n",
    "\n",
    "    variaveis = list(NORMAL_VALUES.keys())\n",
    "    df_imputado = imputar_batch_simples(df_batch, variaveis, NORMAL_VALUES)\n",
    "    print(f\"Imputação forward-fill simples concluída para batch com {len(batch_stay_ids)} stays. Linhas após imputação: {len(df_imputado)}\")\n",
    "\n",
    "    return df_imputado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65694003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_batch_labs(batch_stay_ids):\n",
    "    batch_ids_str = \",\".join(map(str, batch_stay_ids))\n",
    "    query_labs = f\"\"\"\n",
    "    SELECT\n",
    "        icu.stay_id,\n",
    "        le.charttime,\n",
    "        le.itemid,\n",
    "        le.valuenum\n",
    "    FROM mimiciv_hosp.labevents le\n",
    "    JOIN mimiciv_icu.icustays icu ON le.subject_id = icu.subject_id AND le.hadm_id = icu.hadm_id1\n",
    "    WHERE le.itemid IN ({','.join(map(str, itemids_labs))})\n",
    "      AND icu.stay_id IN (SELECT stay_id FROM todas_utis)\n",
    "      AND le.valuenum IS NOT NULL\n",
    "      AND icu.stay_id IN ({batch_ids_str})\n",
    "    ORDER BY icu.stay_id, le.charttime;\n",
    "    \"\"\"\n",
    "    print(f\"Consultando exames laboratoriais batch com {len(batch_stay_ids)} stays...\")\n",
    "    df_labs = pd.read_sql(query_labs, conn)\n",
    "    df_labs['charttime'] = pd.to_datetime(df_labs['charttime'])\n",
    "    print(f\"Consulta concluída. Linhas: {len(df_labs)}\")\n",
    "\n",
    "    # Pivotar para formato wide: uma linha por (stay_id, charttime) com colunas lab_<itemid>\n",
    "    df_pivot = df_labs.pivot_table(index=['stay_id', 'charttime'], columns='itemid', values='valuenum')\n",
    "    df_pivot.columns = [f'lab_{col}' for col in df_pivot.columns]\n",
    "    df_pivot = df_pivot.reset_index()\n",
    "    df_pivot = df_pivot.sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "\n",
    "    # Variáveis laboratorias para imputar\n",
    "    variaveis = [col for col in df_pivot.columns if col.startswith('lab_')]\n",
    "\n",
    "    # Imputação forward-fill simples conforme artigo\n",
    "    df_imputado = imputar_batch_simples(df_pivot, variaveis, NORMAL_VALUES)\n",
    "\n",
    "    print(f\"Imputação forward-fill simples concluída para batch labs com {len(batch_stay_ids)} stays. Linhas após imputação: {len(df_imputado)}\")\n",
    "\n",
    "    return df_imputado\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50f3a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_batch_processed(batch_idx, output_dir):\n",
    "    done_flag = os.path.join(output_dir, f'batch_{batch_idx:04d}.parquet.done')\n",
    "    return os.path.exists(done_flag)\n",
    "\n",
    "def save_batch(df, batch_idx, output_dir):\n",
    "    filename = os.path.join(output_dir, f'batch_{batch_idx:04d}.parquet')\n",
    "    df.to_parquet(filename)\n",
    "    with open(filename + '.done', 'w') as f:\n",
    "        f.write('done')\n",
    "    print(f\"Batch {batch_idx} salvo e marcado como concluído.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c44e4327",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/619788631.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_labs = pd.read_sql(query_labs, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch vitals 0 já processado, pulando...\n",
      "Batch vitals 1 já processado, pulando...\n",
      "Batch vitals 2 já processado, pulando...\n",
      "Batch vitals 3 já processado, pulando...\n",
      "Batch vitals 4 já processado, pulando...\n",
      "Batch vitals 5 já processado, pulando...\n",
      "Batch vitals 6 já processado, pulando...\n",
      "Batch vitals 7 já processado, pulando...\n",
      "Batch labs 0 já processado, pulando...\n",
      "Batch labs 1 já processado, pulando...\n",
      "Consultando exames laboratoriais batch com 10000 stays...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta concluída. Linhas: 1707642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/2559897179.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(df_imputado_list, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputação forward-fill simples concluída para batch labs com 10000 stays. Linhas após imputação: 28414322\n",
      "Batch 2 salvo e marcado como concluído.\n",
      "Consultando exames laboratoriais batch com 10000 stays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/619788631.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_labs = pd.read_sql(query_labs, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta concluída. Linhas: 1702969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/2559897179.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(df_imputado_list, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputação forward-fill simples concluída para batch labs com 10000 stays. Linhas após imputação: 28303511\n",
      "Batch 3 salvo e marcado como concluído.\n",
      "Consultando exames laboratoriais batch com 10000 stays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/619788631.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_labs = pd.read_sql(query_labs, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta concluída. Linhas: 1768708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/2559897179.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(df_imputado_list, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputação forward-fill simples concluída para batch labs com 10000 stays. Linhas após imputação: 29229610\n",
      "Batch 4 salvo e marcado como concluído.\n",
      "Consultando exames laboratoriais batch com 10000 stays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/619788631.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_labs = pd.read_sql(query_labs, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta concluída. Linhas: 1745110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/2559897179.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(df_imputado_list, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputação forward-fill simples concluída para batch labs com 10000 stays. Linhas após imputação: 28633305\n",
      "Batch 5 salvo e marcado como concluído.\n",
      "Consultando exames laboratoriais batch com 10000 stays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/619788631.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_labs = pd.read_sql(query_labs, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta concluída. Linhas: 1740356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/2559897179.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(df_imputado_list, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputação forward-fill simples concluída para batch labs com 10000 stays. Linhas após imputação: 28757807\n",
      "Batch 6 salvo e marcado como concluído.\n",
      "Consultando exames laboratoriais batch com 3073 stays...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/619788631.py:18: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_labs = pd.read_sql(query_labs, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consulta concluída. Linhas: 543186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_102500/2559897179.py:82: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat(df_imputado_list, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputação forward-fill simples concluída para batch labs com 3073 stays. Linhas após imputação: 8887413\n",
      "Batch 7 salvo e marcado como concluído.\n"
     ]
    }
   ],
   "source": [
    "batches = [todas_utis[i:i+BATCH_SIZE] for i in range(0, len(todas_utis), BATCH_SIZE)]\n",
    "\n",
    "# Processar batches vitals\n",
    "for batch_idx, batch_stays in enumerate(batches):\n",
    "    if is_batch_processed(batch_idx, OUTPUT_DIR_VITALS):\n",
    "        print(f\"Batch vitals {batch_idx} já processado, pulando...\")\n",
    "        continue\n",
    "    df_imputado = processar_batch_vitals(batch_stays)\n",
    "    save_batch(df_imputado, batch_idx, OUTPUT_DIR_VITALS)\n",
    "\n",
    "# Processar batches labs\n",
    "for batch_idx, batch_stays in enumerate(batches):\n",
    "    if is_batch_processed(batch_idx, OUTPUT_DIR_LABS):\n",
    "        print(f\"Batch labs {batch_idx} já processado, pulando...\")\n",
    "        continue\n",
    "    df_imputado_labs = processar_batch_labs(batch_stays)\n",
    "    save_batch(df_imputado_labs, batch_idx, OUTPUT_DIR_LABS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3359fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_vasopressores_e_marcar_falencia(df_merge):\n",
    "    print(f\"✅ Iniciando processamento dos vasopressores e marcação de falência.\")\n",
    "\n",
    "    query_vasoact = \"\"\"\n",
    "    SELECT \n",
    "        stay_id,\n",
    "        starttime,\n",
    "        endtime,\n",
    "        dopamine,\n",
    "        epinephrine,\n",
    "        norepinephrine,\n",
    "        phenylephrine,\n",
    "        vasopressin,\n",
    "        dobutamine,\n",
    "        milrinone\n",
    "    FROM mimiciv_derived.vasoactive_agent\n",
    "    WHERE stay_id IN (SELECT DISTINCT stay_id FROM todas_utis);\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Consultando dados de vasopressores...\")\n",
    "\n",
    "    df_vasoact = pd.read_sql(query_vasoact, conn)\n",
    "    df_vasoact['starttime'] = pd.to_datetime(df_vasoact['starttime'])\n",
    "    df_vasoact['endtime'] = pd.to_datetime(df_vasoact['endtime'])\n",
    "\n",
    "    vaso_cols = ['dopamine', 'epinephrine', 'norepinephrine', 'phenylephrine',\n",
    "                 'vasopressin', 'dobutamine', 'milrinone']\n",
    "\n",
    "    print(f\"✅ Dados de vasopressores carregados. Total de registros: {len(df_vasoact)}\")\n",
    "\n",
    "    df_merge['stay_id'] = df_merge['stay_id'].astype(int)\n",
    "    df_merge['charttime'] = pd.to_datetime(df_merge['charttime'])\n",
    "    df_vasoact['stay_id'] = df_vasoact['stay_id'].astype(int)\n",
    "\n",
    "    df_merge = df_merge.dropna(subset=['charttime'])\n",
    "    df_vasoact = df_vasoact.dropna(subset=['starttime'])\n",
    "\n",
    "    print(f\"✅ Dados preparados: {len(df_merge)} stays vitais e {len(df_vasoact)} stays de vasopressores.\")\n",
    "\n",
    "    df_merge = df_merge.sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "    df_vasoact = df_vasoact.sort_values(['stay_id', 'starttime']).reset_index(drop=True)\n",
    "\n",
    "    dfs_merged = []\n",
    "\n",
    "    print(f\"✅ Iniciando merge dos dados de vasopressores com sinais vitais...\")\n",
    "\n",
    "    for sid in df_merge['stay_id'].unique():\n",
    "        print(f\"Processando stay_id {sid}...\")\n",
    "\n",
    "        df_sid = df_merge[df_merge['stay_id'] == sid]\n",
    "        df_vaso_sid = df_vasoact[df_vasoact['stay_id'] == sid]\n",
    "\n",
    "        if df_vaso_sid.empty:\n",
    "            print(f\"⚠️ Sem dados de vasopressores para stay_id {sid}, preenchendo com NA.\")\n",
    "            df_sid.loc[:, ['starttime', 'endtime'] + vaso_cols] = pd.NA\n",
    "            dfs_merged.append(df_sid)\n",
    "            continue\n",
    "\n",
    "        merged_sid = pd.merge_asof(\n",
    "            df_sid, df_vaso_sid[['stay_id', 'starttime', 'endtime'] + vaso_cols],\n",
    "            left_on='charttime', right_on='starttime',\n",
    "            by='stay_id', direction='backward', tolerance=pd.Timedelta('2D')\n",
    "        )\n",
    "        dfs_merged.append(merged_sid)\n",
    "\n",
    "    df_vaso_merged = pd.concat(dfs_merged, ignore_index=True)\n",
    "\n",
    "    print(f\"✅ Merge concluído. Total de registros após merge: {len(df_vaso_merged)}\")\n",
    "\n",
    "    # Condições para marcação da falência\n",
    "    cond_tempo = (df_vaso_merged['charttime'] >= df_vaso_merged['starttime']) & \\\n",
    "                 (df_vaso_merged['charttime'] <= df_vaso_merged['endtime'])\n",
    "    cond_vaso = df_vaso_merged[vaso_cols].notna().any(axis=1)\n",
    "    cond_mbp = df_vaso_merged['mbp'] < 65\n",
    "    cond_lactato = df_vaso_merged.get('lab_50813', pd.Series(0)) >= 2  # lactato lab_50813\n",
    "\n",
    "    print(f\"✅ Aplicando condições para marcar falência...\")\n",
    "\n",
    "    df_vaso_merged['falencia'] = 0\n",
    "    idx_falencia = df_vaso_merged.index[cond_tempo & cond_vaso & (cond_mbp | cond_lactato)]\n",
    "    df_vaso_merged.loc[idx_falencia, 'falencia'] = 1\n",
    "\n",
    "    print(f\"✅ Falência circulatória marcada. Total casos: {df_vaso_merged['falencia'].sum()}\")\n",
    "\n",
    "    return df_vaso_merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c91d2aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  database \"mimiciv\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.35, but the operating system provides version 2.39.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE mimiciv REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n",
      "/tmp/ipykernel_103629/3746042496.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_vasoact = pd.read_sql(query_vasoact, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexão ao banco estabelecida.\n",
      "✅ Iniciando processamento dos vasopressores e marcação de falência.\n",
      "Consultando dados de vasopressores...\n",
      "✅ Dados de vasopressores carregados. Total de registros: 665528\n",
      "✅ Dados preparados: 35225737 stays vitais e 665528 stays de vasopressores.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel deu pane ao executar o código na célula atual ou em uma célula anterior. \n",
      "\u001b[1;31mAnalise o código nas células para identificar uma possível causa da pane. \n",
      "\u001b[1;31mClique <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqui</a> para obter mais informações. \n",
      "\u001b[1;31mConsulte Jupyter <a href='command:jupyter.viewOutput'>log</a> para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mimiciv\",\n",
    "    user=\"uti_user\",\n",
    "    password=\"s0f4C1#4\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "print(\"✅ Conexão ao banco estabelecida.\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"ROLLBACK\")  # para garantir estado limpo\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TEMP TABLE todas_utis AS\n",
    "SELECT \n",
    "    i.subject_id,\n",
    "    i.hadm_id,\n",
    "    i.stay_id,\n",
    "    i.intime,\n",
    "    i.outtime,\n",
    "    EXTRACT(EPOCH FROM (i.outtime - i.intime)) / 60 AS duracao_minutos\n",
    "FROM mimiciv_icu.icustays i\n",
    "WHERE EXTRACT(EPOCH FROM (i.outtime - i.intime)) > 3600;\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "\n",
    "df_falencia = processar_vasopressores_e_marcar_falencia(df_merge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86127c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:  database \"mimiciv\" has a collation version mismatch\n",
      "DETAIL:  The database was created using collation version 2.35, but the operating system provides version 2.39.\n",
      "HINT:  Rebuild all objects in this database that use the default collation and run ALTER DATABASE mimiciv REFRESH COLLATION VERSION, or build PostgreSQL with the right library version.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Conexão ao banco estabelecida.\n",
      "\n",
      "🔁 Processando batch 0...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import gc\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"mimiciv\",\n",
    "    user=\"uti_user\",\n",
    "    password=\"s0f4C1#4\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "print(\"✅ Conexão ao banco estabelecida.\")\n",
    "\n",
    "# Ajuste os caminhos conforme necessário\n",
    "OUTPUT_DIR_VITALS = 'output/vitals_batches'\n",
    "OUTPUT_DIR_LABS = 'output/labs_batches'\n",
    "OUTPUT_DIR_FALENCIA = 'output/falencia_batches'\n",
    "\n",
    "os.makedirs(OUTPUT_DIR_FALENCIA, exist_ok=True)\n",
    "\n",
    "for batch_idx in range(8):  # 0 a 7\n",
    "    print(f\"\\n🔁 Processando batch {batch_idx}...\")\n",
    "\n",
    "    vitals_path = os.path.join(OUTPUT_DIR_VITALS, f'batch_{batch_idx:04d}.parquet')\n",
    "    labs_path = os.path.join(OUTPUT_DIR_LABS, f'batch_{batch_idx:04d}.parquet')\n",
    "    falencia_path = os.path.join(OUTPUT_DIR_FALENCIA, f'batch_{batch_idx:04d}.parquet')\n",
    "\n",
    "    if not os.path.exists(vitals_path) or not os.path.exists(labs_path):\n",
    "        print(f\"❌ Arquivo ausente no batch {batch_idx}, pulando.\")\n",
    "        continue\n",
    "\n",
    "    # Carregar os arquivos\n",
    "    df_v = pd.read_parquet(vitals_path)\n",
    "    df_l = pd.read_parquet(labs_path)\n",
    "\n",
    "    # Merge vitals + labs\n",
    "    df_merge = pd.merge(df_v, df_l, on=['stay_id', 'charttime'], how='outer')\n",
    "    df_merge = df_merge.sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # Salvar resultado\n",
    "    df_falencia.to_parquet(falencia_path)\n",
    "    print(f\"✅ Batch {batch_idx} salvo com falência. Linhas: {len(df_falencia)}\")\n",
    "\n",
    "    # Liberação de memória\n",
    "    del df_v, df_l, df_merge, df_falencia\n",
    "    gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40d3e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f82904ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_features_instabilidade(df, event_col='falencia'):\n",
    "    df = df.sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "    df['estado_atual'] = df[event_col]\n",
    "    df['tempo_desde_ultimo_evento'] = np.nan\n",
    "\n",
    "    for stay_id, grupo in df.groupby('stay_id'):\n",
    "        estados = grupo['estado_atual'].values\n",
    "        tempos = grupo['charttime'].values\n",
    "\n",
    "        last_event_time = None\n",
    "        tempo_desde = []\n",
    "\n",
    "        for i, estado in enumerate(estados):\n",
    "            if estado == 1:\n",
    "                last_event_time = tempos[i]\n",
    "                tempo_desde.append(0)\n",
    "            else:\n",
    "                if last_event_time is None:\n",
    "                    tempo_desde.append(np.nan)\n",
    "                else:\n",
    "                    delta = (tempos[i] - last_event_time).astype('timedelta64[m]').astype(float)\n",
    "                    tempo_desde.append(delta)\n",
    "\n",
    "        df.loc[grupo.index, 'tempo_desde_ultimo_evento'] = tempo_desde\n",
    "\n",
    "    df['duracao_evento'] = df.groupby('stay_id')['estado_atual'].transform(lambda x: x.expanding().mean())\n",
    "\n",
    "    print(\"✅ Features de instabilidade criadas.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18e25149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_features_intensidade(df, vars_continuas):\n",
    "    df = df.sort_values(['stay_id', 'charttime']).reset_index(drop=True)\n",
    "\n",
    "    novas_colunas = {}\n",
    "\n",
    "    for var in vars_continuas:\n",
    "        mask = ~df[var].isna()\n",
    "\n",
    "        tempo_desde_ultima_medicao = np.full(len(df), np.nan)\n",
    "        prop_medicoes = np.full(len(df), np.nan)\n",
    "\n",
    "        for stay_id, grupo in df.groupby('stay_id'):\n",
    "            tempos = grupo['charttime'].values\n",
    "            mask_var = mask.loc[grupo.index].values\n",
    "\n",
    "            last_meas_time = None\n",
    "            tempos_desde = []\n",
    "            contagem = 0\n",
    "\n",
    "            for i, presente in enumerate(mask_var):\n",
    "                if presente:\n",
    "                    last_meas_time = tempos[i]\n",
    "                    contagem += 1\n",
    "                    tempos_desde.append(0)\n",
    "                else:\n",
    "                    if last_meas_time is None:\n",
    "                        tempos_desde.append(np.nan)\n",
    "                    else:\n",
    "                        delta = (tempos[i] - last_meas_time).astype('timedelta64[m]').astype(float)\n",
    "                        tempos_desde.append(delta)\n",
    "\n",
    "            prop_medicoes_grupo = [contagem / (i + 1) for i in range(len(tempos))]\n",
    "\n",
    "            tempo_desde_ultima_medicao[grupo.index] = tempos_desde\n",
    "            prop_medicoes[grupo.index] = prop_medicoes_grupo\n",
    "\n",
    "        novas_colunas[f'{var}_tempo_desde_ultima_medicao'] = tempo_desde_ultima_medicao\n",
    "        novas_colunas[f'{var}_prop_medicoes'] = prop_medicoes\n",
    "\n",
    "    df_novas = pd.DataFrame(novas_colunas, index=df.index)\n",
    "    df = pd.concat([df, df_novas], axis=1)\n",
    "\n",
    "    print(\"✅ Features de intensidade de medição criadas.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe1c011",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_features_cumulativas(df, exclude_cols=None, id_col='stay_id', time_col='charttime'):\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "\n",
    "    cols = [c for c in df.columns if c not in exclude_cols + [id_col, time_col] and not c.endswith('_mask')]\n",
    "    df = df.sort_values([id_col, time_col]).reset_index(drop=True)\n",
    "    df_feat = df.copy()\n",
    "\n",
    "    for col in cols:\n",
    "        df_feat[f'min_{col}'] = df_feat.groupby(id_col)[col].cummin().ffill()\n",
    "        df_feat[f'max_{col}'] = df_feat.groupby(id_col)[col].cummax().ffill()\n",
    "\n",
    "        n_meas = df_feat.groupby(id_col)[col].apply(lambda x: x.notna().cumsum()).reset_index(level=0, drop=True)\n",
    "        cumsum = df_feat.groupby(id_col)[col].cumsum().ffill()\n",
    "\n",
    "        df_feat[f'n_meas_{col}'] = n_meas\n",
    "        df_feat[f'mean_{col}'] = cumsum / n_meas.replace(0, pd.NA)\n",
    "        df_feat[f'mean_{col}'] = df_feat[f'mean_{col}'].fillna(method='ffill')\n",
    "\n",
    "    print(\"✅ Features cumulativas extraídas.\")\n",
    "    return df_feat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ccc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_completo = processar_vasopressores_e_marcar_falencia(df_completo)\n",
    "\n",
    "vars_validas = [var for var in vars_continuas if df_completo[var].isna().mean() < 0.5]\n",
    "masks_validas = [v + '_mask' for v in vars_validas]\n",
    "\n",
    "colunas_finais = vars_validas + masks_validas + ['stay_id', 'charttime', 'falencia']\n",
    "df_final = df_completo[colunas_finais].copy()\n",
    "\n",
    "print(f\"✅ Dataset final pronto com {len(vars_validas)} variáveis contínuas válidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88a60a63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_completo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 44\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJanelas rejeitadas por NaN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrejeitadas_nan\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrejeitadas_nan\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal_janelas\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y), stays, times\n\u001b[0;32m---> 44\u001b[0m X, y, stays, times \u001b[38;5;241m=\u001b[39m construir_janelas_temporais(\u001b[43mdf_completo\u001b[49m, max_nan_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Janelas temporais criadas. Total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, X shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, y shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_completo' is not defined"
     ]
    }
   ],
   "source": [
    "def construir_janelas_temporais(df, jan_obs=36, jan_pred=12, passo=1, max_nan_ratio=0.5):\n",
    "    candidate_cols = [\n",
    "        col for col in df.columns\n",
    "        if col not in ['stay_id', 'charttime', 'falencia'] and not col.endswith('_mask')\n",
    "    ]\n",
    "    vars_features = [col for col in candidate_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "    if len(vars_features) == 0:\n",
    "        raise ValueError(\"Nenhuma variável numérica válida para construir janelas.\")\n",
    "\n",
    "    X, y, stays, times = [], [], [], []\n",
    "\n",
    "    total_janelas = 0\n",
    "    rejeitadas_nan = 0\n",
    "\n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        group = group.reset_index(drop=True)\n",
    "        count_validas = 0\n",
    "\n",
    "        max_start = len(group) - (jan_obs + jan_pred) + 1\n",
    "        for i in range(0, max_start, passo):\n",
    "            janela_obs = group.iloc[i:i + jan_obs]\n",
    "            janela_pred = group.iloc[i + jan_obs:i + jan_obs + jan_pred]\n",
    "\n",
    "            nan_ratio = janela_obs[vars_features].isna().mean().mean()\n",
    "            total_janelas += 1\n",
    "\n",
    "            if nan_ratio > max_nan_ratio:\n",
    "                rejeitadas_nan += 1\n",
    "                continue\n",
    "\n",
    "            X.append(janela_obs[vars_features].values)\n",
    "            y.append(int(janela_pred['falencia'].any()))\n",
    "            stays.append(stay_id)\n",
    "            times.append(janela_obs['charttime'].iloc[0])\n",
    "            count_validas += 1\n",
    "\n",
    "    print(f\"\\nTotal janelas avaliadas: {total_janelas}\")\n",
    "    print(f\"Janelas rejeitadas por NaN: {rejeitadas_nan} ({rejeitadas_nan / total_janelas:.2%})\")\n",
    "\n",
    "    return np.array(X), np.array(y), stays, times\n",
    "\n",
    "\n",
    "X, y, stays, times = construir_janelas_temporais(df_completo, max_nan_ratio=0.5)\n",
    "print(f\"✅ Janelas temporais criadas. Total: {len(X)}, X shape: {X.shape}, y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7a5439",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_shape = X.shape\n",
    "X_flat = X.reshape(-1, X_shape[2])\n",
    "\n",
    "X_flat = np.nan_to_num(X_flat, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "X_flat = X_flat.astype(np.float64)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_flat).reshape(X_shape)\n",
    "\n",
    "X = X_scaled\n",
    "\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "print(\"✅ Padronização z-score aplicada.\")\n",
    "\n",
    "np.save(\"X.npy\", X)\n",
    "np.save(\"y.npy\", y)\n",
    "pd.DataFrame({\"stay_id\": stays, \"start_time\": times}).to_csv(\"janelas_metadata.csv\", index=False)\n",
    "print(\"✅ Dados exportados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WSL)",
   "language": "python",
   "name": "wsl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
